{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from notebook_helper import MyCorpus, build_hybrid_model, build_callbacks, build_embedding_matrix, iteration_features, show_stats\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from string import ascii_letters\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "# Keras specific\n",
    "\n",
    "#### CHANGED from import keras:\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "#####\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, CuDNNLSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# Gemsim\n",
    "import gensim.models\n",
    "from gensim import utils\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "run_random_forests = True\n",
    "cap_at_one = False\n",
    "drop_sibling_index = True\n",
    "drop_contains = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Before:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    print(\"After:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "except IndexError as e:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (18293, 57)\n",
      "Number of parameter vecs:\t\t18293\n",
      "without logging (negatives):\t17987\n",
      "with logging (positives):\t\t306\n",
      "Log ratio:\t\t\t\t\t\t1.67%\n",
      "   type  count  positives     ratio\n",
      "9     m    849         64  0.075383\n",
      "4     h    181          4  0.022099\n",
      "2     e   6934        129  0.018604\n",
      "8     l   1727         31  0.017950\n",
      "6     j    166          2  0.012048\n",
      "7     k   1162         14  0.012048\n",
      "5     i    961         11  0.011446\n",
      "1     d   4321         47  0.010877\n",
      "3     f   1171          4  0.003416\n",
      "0     c    644          0  0.000000\n",
      "10    o    177          0  0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "       location  length  num_siblings  sibling_index  num_children  \\\n0  106;0-129;23      24            12             11             3   \n1  134;4-139;29       6             3              2             3   \n2    15;0-17;50       3            12              7             3   \n3    19;0-61;18      43            12              8             3   \n4    64;0-68;51       5            12              9             3   \n\n   depth_from_def                                            context  \\\n0               0                              cqqdBdBdqirrrrmqrerrr   \n1               0                                            cderrrr   \n2               1                                     azzziAmAqqdurr   \n3               1  azzziAmAqqdurrdeqrrerueruferuerqrrqrqrqrfrerukruu   \n4               1  azzziAmAqqdurrdeqrrerueruferuerqrrqrqrqrfreruk...   \n\n   contains_class_definition  contains_function_definition  \\\n0                          0                             3   \n1                          0                             1   \n2                          0                             0   \n3                          0                             0   \n4                          0                             0   \n\n   contains_if_statement  ...  parent_d  parent_e  parent_f  parent_h  \\\n0                      0  ...         0         0         0         0   \n1                      0  ...         1         0         0         0   \n2                      0  ...         0         0         0         0   \n3                      4  ...         0         0         0         0   \n4                      0  ...         0         0         0         0   \n\n   parent_i  parent_j  parent_k  parent_l  parent_m  parent_o  \n0         0         0         0         0         0         0  \n1         0         0         0         0         0         0  \n2         0         0         0         0         0         0  \n3         0         0         0         0         0         0  \n4         0         0         0         0         0         0  \n\n[5 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>location</th>\n      <th>length</th>\n      <th>num_siblings</th>\n      <th>sibling_index</th>\n      <th>num_children</th>\n      <th>depth_from_def</th>\n      <th>context</th>\n      <th>contains_class_definition</th>\n      <th>contains_function_definition</th>\n      <th>contains_if_statement</th>\n      <th>...</th>\n      <th>parent_d</th>\n      <th>parent_e</th>\n      <th>parent_f</th>\n      <th>parent_h</th>\n      <th>parent_i</th>\n      <th>parent_j</th>\n      <th>parent_k</th>\n      <th>parent_l</th>\n      <th>parent_m</th>\n      <th>parent_o</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106;0-129;23</td>\n      <td>24</td>\n      <td>12</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>cqqdBdBdqirrrrmqrerrr</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>134;4-139;29</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>cderrrr</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15;0-17;50</td>\n      <td>3</td>\n      <td>12</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n      <td>azzziAmAqqdurr</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19;0-61;18</td>\n      <td>43</td>\n      <td>12</td>\n      <td>8</td>\n      <td>3</td>\n      <td>1</td>\n      <td>azzziAmAqqdurrdeqrrerueruferuerqrrqrqrqrfrerukruu</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64;0-68;51</td>\n      <td>5</td>\n      <td>12</td>\n      <td>9</td>\n      <td>3</td>\n      <td>1</td>\n      <td>azzziAmAqqdurrdeqrrerueruferuerqrrqrqrqrfreruk...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "min_val = 50\n",
    "# repo_name = f\"174_min{min_val}_expanded\"\n",
    "repo_name = f\"174_min{min_val}_zhenhao\"\n",
    "# repo_name = \"web2py\"\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../features/'+ repo_name + \".csv\")\n",
    "\n",
    "# Remove errors\n",
    "df = df[df.parent != 'b']\n",
    "df = df[df.type != 'b']\n",
    "\n",
    "# Onehot encode type and parent\n",
    "df = pd.get_dummies(df, columns=[\"type\", \"parent\"])\n",
    "\n",
    "show_stats(df)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# df['indentation'] = list(map(lambda x: int(x.split(';')[1]) // 4, np.array(list(df.location.str.split('-')))[:,0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if cap_at_one:\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"contains\"):\n",
    "            df[col] = df[col].apply(lambda x: min(1, x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "       location  length  num_siblings  sibling_index  num_children  \\\n0  106;0-129;23      24            12             11             3   \n1  134;4-139;29       6             3              2             3   \n2    15;0-17;50       3            12              7             3   \n3    19;0-61;18      43            12              8             3   \n4    64;0-68;51       5            12              9             3   \n\n   depth_from_def                                            context  \\\n0               0  [2, 16, 16, 3, 27, 3, 27, 3, 16, 8, 17, 17, 17...   \n1               0                          [2, 3, 4, 17, 17, 17, 17]   \n2               1  [0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...   \n3               1  [0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...   \n4               1  [0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...   \n\n   contains_class_definition  contains_function_definition  \\\n0                          0                             3   \n1                          0                             1   \n2                          0                             0   \n3                          0                             0   \n4                          0                             0   \n\n   contains_if_statement  ...  parent_d  parent_e  parent_f  parent_h  \\\n0                      0  ...         0         0         0         0   \n1                      0  ...         1         0         0         0   \n2                      0  ...         0         0         0         0   \n3                      4  ...         0         0         0         0   \n4                      0  ...         0         0         0         0   \n\n   parent_i  parent_j  parent_k  parent_l  parent_m  parent_o  \n0         0         0         0         0         0         0  \n1         0         0         0         0         0         0  \n2         0         0         0         0         0         0  \n3         0         0         0         0         0         0  \n4         0         0         0         0         0         0  \n\n[5 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>location</th>\n      <th>length</th>\n      <th>num_siblings</th>\n      <th>sibling_index</th>\n      <th>num_children</th>\n      <th>depth_from_def</th>\n      <th>context</th>\n      <th>contains_class_definition</th>\n      <th>contains_function_definition</th>\n      <th>contains_if_statement</th>\n      <th>...</th>\n      <th>parent_d</th>\n      <th>parent_e</th>\n      <th>parent_f</th>\n      <th>parent_h</th>\n      <th>parent_i</th>\n      <th>parent_j</th>\n      <th>parent_k</th>\n      <th>parent_l</th>\n      <th>parent_m</th>\n      <th>parent_o</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>106;0-129;23</td>\n      <td>24</td>\n      <td>12</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[2, 16, 16, 3, 27, 3, 27, 3, 16, 8, 17, 17, 17...</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>134;4-139;29</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[2, 3, 4, 17, 17, 17, 17]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15;0-17;50</td>\n      <td>3</td>\n      <td>12</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19;0-61;18</td>\n      <td>43</td>\n      <td>12</td>\n      <td>8</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64;0-68;51</td>\n      <td>5</td>\n      <td>12</td>\n      <td>9</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[0, 25, 25, 25, 8, 26, 12, 26, 16, 16, 3, 20, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the compacted context from letters into strings of integers\n",
    "df.context = [list(map(lambda y: str(ascii_letters.index(y)), list(str(x)))) for x in df.context]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "if drop_sibling_index:\n",
    "    X = df.drop([\"location\", \"contains_logging\", \"sibling_index\"], axis=1)\n",
    "else:\n",
    "    X = df.drop([\"location\", \"contains_logging\"], axis=1)\n",
    "y = df.contains_logging\n",
    "\n",
    "# Drop the contains_features for comparison with Zhenhao\n",
    "if drop_contains:\n",
    "    drop_cols = []\n",
    "    for col in X:\n",
    "        col_str = str(col)\n",
    "        if not col_str.startswith(\"contains\"):\n",
    "            continue\n",
    "        drop_cols.append(col_str)\n",
    "    X = X.drop(drop_cols, axis=1)\n",
    "\n",
    "# Keep holdout set for testing after k-fold cross validation\n",
    "X, X_holdout, y, y_holdout = train_test_split(X, y, test_size = 0.1, stratify=y, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "score_names = [\n",
    "    \"Balanced accuracy score\",\n",
    "    \"Precision score\",\n",
    "    \"Recall score\",\n",
    "    \"F1 Binary\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Folded Random Forest Run\n",
    "if run_random_forests:\n",
    "    X_rnd = X.drop([\"context\"], axis=1)\n",
    "    rnd_forest_sampling_strategy = 0.05\n",
    "    use_sampling = False\n",
    "    n_splits=10\n",
    "    rnd_repo_name = repo_name + (\"_over\" if use_sampling else \"\") + \\\n",
    "                    (\"_drop_sibling_index\" if drop_sibling_index else \"\") + \\\n",
    "                    (\"_drop_contains\" if drop_contains else \"\") + f\"_cv{n_splits}\"\n",
    "    # Hyper Params\n",
    "    n_estimators = 50\n",
    "    min_samples_split = 5\n",
    "    min_samples_leaf = 1\n",
    "    max_depth = None\n",
    "    class_weight = {False: 1, True: 4}\n",
    "\n",
    "    # List of the k classifiers from the k-fold split\n",
    "    # classifiers = []\n",
    "\n",
    "    df_regular = df.drop([\"context\"], axis=1)\n",
    "    dataframes = [\n",
    "        (\"regular\", df_regular),\n",
    "        # (\"corpus\", df_corpus_embeddings.join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"tf-idf\", df_tfidf.join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"regular+corpus\", df_regular.join(df_corpus_embeddings)),\n",
    "        # (\"regular+tfidf\", df_regular.join(df_tfidf)),\n",
    "        # (\"corpus+tfidf\", df_corpus_embeddings.join(df_tfidf).join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"regular+corpus+tfidf\", df_regular.join(df_corpus_embeddings).join(df_tfidf)),\n",
    "    ]\n",
    "\n",
    "    # true_weight = 4\n",
    "    # for df_name, df_used in dataframes:\n",
    "    #     print(df_name)\n",
    "    # for true_weight in [2,3,3.5,4,4.5,5]:\n",
    "    for true_weight in [4]:\n",
    "    # for n_estimators in [31,50,100]:\n",
    "        class_weight = {False: 1, True: true_weight}\n",
    "        print(f\"Weight: {class_weight[True]} Estimators: {n_estimators}\")\n",
    "        all_scores = []\n",
    "        conf_matrices = []\n",
    "        # Split data into train and test sets\n",
    "        # X = df_used.drop([\"contains_logging\", \"location\"], axis=1)\n",
    "        # if \"regular\" in df_name:\n",
    "        #     X = pd.get_dummies(X, columns=[\"type\", \"parent\"])\n",
    "        # y = df_used.contains_logging\n",
    "\n",
    "        # classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "        # classifier = RandomForestClassifier(n_estimators=9, random_state=0)\n",
    "        # skf = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25)\n",
    "        for k_fold, (train_index, test_index) in enumerate(skf.split(X_rnd, y)):\n",
    "            print(f\"Starting fold {k_fold + 1}.\")\n",
    "            classifier = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           max_depth=max_depth,\n",
    "                                           class_weight=class_weight)\n",
    "            X_train, X_test = X_rnd.iloc[train_index], X_rnd.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            # Sampling\n",
    "            if use_sampling:\n",
    "                # sampler = RandomUnderSampler(sampling_strategy=rnd_forest_sampling_strategy)\n",
    "                sampler = RandomOverSampler(sampling_strategy=rnd_forest_sampling_strategy)\n",
    "                X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            # classifiers.append(classifier)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            scores = [\n",
    "                    balanced_accuracy_score(y_test, y_pred),\n",
    "                    precision_score(y_test, y_pred),\n",
    "                    recall_score(y_test, y_pred),\n",
    "                    f1_score(y_test, y_pred, average='binary', pos_label=True)\n",
    "            ]\n",
    "            all_scores.append(scores)\n",
    "            cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "            conf_matrices.append(cm)\n",
    "        score_df = pd.DataFrame(all_scores, columns=score_names).mean().round(3)\n",
    "\n",
    "        # Testing with the holdout set, slightly different results but ultimately useless\n",
    "        # X_rnd_holdout = X_holdout.drop([\"context\"], axis=1)\n",
    "        # holdout_scores = []\n",
    "        # for clf in classifiers:\n",
    "        #     y_pred_holdout = clf.predict(X_rnd_holdout)\n",
    "        #     scores = [\n",
    "        #         balanced_accuracy_score(y_holdout, y_pred_holdout),\n",
    "        #         precision_score(y_holdout, y_pred_holdout),\n",
    "        #         recall_score(y_holdout, y_pred_holdout),\n",
    "        #         f1_score(y_holdout, y_pred_holdout, average='binary', pos_label=True)\n",
    "        #     ]\n",
    "        #     holdout_scores.append(scores)\n",
    "\n",
    "        print(score_df)\n",
    "        avg_cm = np.mean(conf_matrices, axis=0).astype(int)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=avg_cm,\n",
    "                                      display_labels=classifier.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.show()\n",
    "        out = open(\"my_approach_rnd_forest_results\", \"a\")\n",
    "        # out.write(\"Name, Timestamp, use_sampling, sampling_strategy, n_estimators, min_samples_split, class_weight, \" + \", \".join(score_names) + \"\\n\")\n",
    "        out.write(f\"{rnd_repo_name}, {time.ctime()}, {use_sampling}, {rnd_forest_sampling_strategy}, {n_estimators}, {min_samples_split}, {class_weight}, \" +\n",
    "                  \", \".join([str(x) for x in score_df.values]) + \"\\n\")\n",
    "        out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Random Forest: Feature importance based on mean decrease in impurity\n",
    "if run_random_forests:\n",
    "    sort_importances = True\n",
    "\n",
    "    importances = classifier.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in classifier.estimators_], axis=0)\n",
    "    if sort_importances:\n",
    "        importances,std,columns = list(zip(*sorted(list(zip(importances, std, X.columns)), reverse=True)))\n",
    "    else:\n",
    "        columns = X.columns\n",
    "\n",
    "    forest_importances = pd.Series(importances, index=columns)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.set_size_inches(18.5, 18.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Random Forest: Feature importance based on feature permutation\n",
    "if run_random_forests and False:\n",
    "    # result = permutation_importance(classifier, X_rnd, y, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "    result = permutation_importance(classifier, X_holdout.drop([\"context\"], axis=1), y_holdout, n_repeats=10, n_jobs=-1)\n",
    "    forest_permutation_importances = pd.Series(result.importances_mean, index=X_rnd.columns)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_permutation_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using permutation on full model\")\n",
    "    ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(18.5, 18.5)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec Model\n",
    "\n",
    "sentences = MyCorpus(list(df.context))\n",
    "gensim_model = gensim.models.Word2Vec(sentences=sentences, min_count=1, workers=14)\n",
    "actual_vocab_size = len(gensim_model.wv.key_to_index)\n",
    "actual_vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Build an embedding for each context as the mean of its words' Word2Vec vectors\n",
    "# corpus_embeddings = []\n",
    "# for doc in sentences:\n",
    "#     doc_embedding = np.zeros((len(doc), 100), dtype=np.float32)\n",
    "#     for idx, word in enumerate(doc):\n",
    "#                     doc_embedding[idx] = gensim_model.wv[word]\n",
    "#     doc_embedding = np.mean(doc_embedding, axis=0)\n",
    "#     corpus_embeddings.append(doc_embedding)\n",
    "# corpus_embeddings = np.array(corpus_embeddings)\n",
    "# df_corpus_embeddings = pd.DataFrame(corpus_embeddings, columns=[\"c\"+str(i) for i in range(100)])\n",
    "# df_corpus_embeddings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# vectorizer = TfidfVectorizer(token_pattern=\"\\w+\")\n",
    "# concat_context = [\" \".join(x) for x in df.context]\n",
    "# vectorize_res = vectorizer.fit_transform(concat_context)\n",
    "# df_tfidf = pd.DataFrame(vectorize_res.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# df_tfidf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Data split for Tensorflow\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=0)\n",
    "# X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# print(\"Negatives: \" + str(y_train[y_train == 0].shape[0]))\n",
    "# print(\"Positives: \" + str(y_train[y_train == 1].shape[0]))\n",
    "# print(\"Ratio:\\t   \" + f\"{y_train[y_train == 1].shape[0] / y_train[y_train == 0].shape[0] * 100:.2f}\" + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Smote doesn't seem to work with the context column\n",
    "# over = SMOTE(random_state=0, sampling_strategy=0.1)\n",
    "# X_train_resampled, y_train_resampled = over.fit_resample(X_train, y_train)\n",
    "\n",
    "# over = RandomOverSampler(random_state=0, sampling_strategy=0.1)\n",
    "# X_train_resampled, y_train_resampled = over.fit_resample(X_train, y_train)\n",
    "\n",
    "# sampling_strategy = 0.05\n",
    "# Choose one:\n",
    "# sampler = RandomUnderSampler(random_state=0, sampling_strategy=sampling_strategy)\n",
    "# sampler = RandomOverSampler(random_state=0, sampling_strategy=sampling_strategy)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# print(\"Negatives: \" + str(y_train_resampled[y_train_resampled == 0].shape[0]))\n",
    "# print(\"Positives: \" + str(y_train_resampled[y_train_resampled == 1].shape[0]))\n",
    "# print(\"Ratio:\\t   \" + f\"{y_train_resampled[y_train_resampled == 1].shape[0] / y_train_resampled[y_train_resampled == 0].shape[0] * 100:.2f}\" + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# X_train, y_train = X_train_resampled, y_train_resampled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 00:58:34.021465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# sampling_strategy = 0.05\n",
    "sampling_strategy = 0.05\n",
    "vocab_size = actual_vocab_size + 1\n",
    "output_dims = 100\n",
    "max_length = 80\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "trainable=True\n",
    "dropout = 0.2\n",
    "val_split = 0.0\n",
    "num_nodes = 128\n",
    "callback = [\"cp\"]\n",
    "callback_monitor = 'val_f1_score'\n",
    "class_weight = \"class_weight_unsupported\"\n",
    "cmpltn_metrics = [tfa.metrics.F1Score(num_classes=1, threshold=0.5)]\n",
    "\n",
    "# Cross-validation settings\n",
    "n_splits = 1\n",
    "\n",
    "settings_hash = abs(hash(str([n_splits, sampling_strategy, vocab_size, output_dims, max_length, num_epochs, batch_size, trainable, dropout, val_split, num_nodes, callback, callback_monitor, class_weight, cmpltn_metrics]))) # TODO: DEPRECATE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = build_embedding_matrix(vocab_size, output_dims, gensim_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# DEPRECATED: Not usable with k-fold cross-validation\n",
    "# Pad the context\n",
    "# padded_context = pad_sequences(np.array(X.context), maxlen=max_length, value=0.0)\n",
    "# Prepare the \"other\" input\n",
    "# other_input = X.drop([\"context\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# Pad the context to create the context input\n",
    "# padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "# padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "# padded_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# Prepare the \"other\" input\n",
    "# regular_inputs = X_train.drop([\"context\", \"location\"], axis=1)\n",
    "# regular_inputs_test = X_test.drop([\"context\", \"location\"], axis=1)\n",
    "# regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "# regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "# regular_inputs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Prepare holdout test sets\n",
    "padded_inputs_holdout = pad_sequences(np.array(list(X_holdout.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "regular_inputs_holdout = X_holdout.drop([\"context\"], axis=1)\n",
    "X_holdout_dict = {\"context\": padded_inputs_holdout, \"other\": regular_inputs_holdout}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1 of 3.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/embedding/embedding_lookup' defined at (most recent call last):\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/4k/y4cn3_x54d51nkdh7ff7yqqm0000gn/T/ipykernel_23967/4165609038.py\", line 69, in <cell line: 44>\n      history = model.fit(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/layers/core/embedding.py\", line 199, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/embedding/embedding_lookup'\nindices[58,67] = 31 is not in [0, 29)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_3415]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 44>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     67\u001B[0m callbacks, model_cp_filepath \u001B[38;5;241m=\u001B[39m build_callbacks(callback, callback_monitor, repo_name, run_folder, k_fold, zhenhao\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[0;32m---> 69\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogging\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m histories\u001B[38;5;241m.\u001B[39mappend(history)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# Predict with final weights\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\nDetected at node 'model/embedding/embedding_lookup' defined at (most recent call last):\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/4k/y4cn3_x54d51nkdh7ff7yqqm0000gn/T/ipykernel_23967/4165609038.py\", line 69, in <cell line: 44>\n      history = model.fit(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/Cellar/python@3.9/3.9.0_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/layers/core/embedding.py\", line 199, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model/embedding/embedding_lookup'\nindices[58,67] = 31 is not in [0, 29)\n\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_3415]"
     ]
    }
   ],
   "source": [
    "run_name = f\"{repo_name}\" + (\"_drop_sibling_index\" if drop_sibling_index else \"\") + \\\n",
    "           (\"_drop_contains\" if drop_contains else \"\") + f\"_cv{n_splits}\"\n",
    "run_number = 1\n",
    "# if not os.path.isdir(f\"hybrid_models\"):\n",
    "if not os.getcwd().endswith(\"notebooks\"):\n",
    "    raise Exception(\"Bad working directory\")\n",
    "while os.path.isdir(f\"hybrid_models/{repo_name}/run{run_number}/\"):\n",
    "    run_number += 1\n",
    "run_folder = f\"run{run_number}\"\n",
    "\n",
    "# DEBUG\n",
    "debug = False\n",
    "if debug:\n",
    "    num_epochs = 1\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "    settings_hash = abs(hash(str(time.ctime())))\n",
    "    run_folder = settings_hash\n",
    "    run_name = f\"DEBUG_{run_name}\"\n",
    "# /DEBUG\n",
    "\n",
    "start = time.time()\n",
    "histories = []\n",
    "\n",
    "# List of (X_test_dict, y_test) of all folds\n",
    "test_sets = []\n",
    "\n",
    "model = build_hybrid_model(vocab_size, output_dims, embedding_matrix, max_length,\n",
    "                       trainable, num_nodes, dropout, X.shape[1] - 1)\n",
    "# model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=cmpltn_metrics)\n",
    "\n",
    "final_bal_acc_all, final_precision_all, final_recall_all, final_f1_all = [], [], [], []\n",
    "best_bal_acc_all, best_precision_all, best_recall_all, best_f1_all = [], [], [], []\n",
    "# K-fold cross-validation\n",
    "if n_splits == 1:\n",
    "    indices = np.arange(y.shape[0])\n",
    "    strat_train_idx, strat_val_idx = train_test_split(indices, test_size=0.25, stratify=y, random_state=0)\n",
    "    idx_iter = [(strat_train_idx, strat_val_idx)]\n",
    "else:\n",
    "    skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=0)\n",
    "    idx_iter = skf.split(X, y)\n",
    "for k_fold, (train_index, test_index) in enumerate(idx_iter):\n",
    "    print(f\"Starting fold {k_fold + 1} of {n_splits}.\")\n",
    "    prog_log = open(\"progess.log\", \"a\")\n",
    "    prog_log.write(f\"{time.ctime()} Starting fold {k_fold + 1} of {n_splits}. Run folder: {run_folder}\\n\")\n",
    "    prog_log.close()\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # Oversample the minority class\n",
    "    sampler = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    # Pad the context to create the context input\n",
    "    padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "    padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "    # Prepare the \"other\" input\n",
    "    regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "    regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "    # Put both inputs into a dict\n",
    "    X_train_dict = {\"context\": padded_inputs, \"other\": regular_inputs}\n",
    "    X_test_dict = {\"context\": padded_inputs_test, \"other\": regular_inputs_test}\n",
    "    # Append to the list of test sets\n",
    "    test_sets.append((X_test_dict, y_test))\n",
    "    # Build the callbacks\n",
    "    callbacks, model_cp_filepath = build_callbacks(callback, callback_monitor, repo_name, run_folder, k_fold, zhenhao=False)\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train_dict,\n",
    "        {\"logging\": y_train},\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_dict, y_test),\n",
    "        validation_split=val_split,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    histories.append(history)\n",
    "\n",
    "    # Predict with final weights\n",
    "    pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "    y_pred = np.round(pred_test)\n",
    "    final_bal_acc_all.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    final_precision_all.append(precision_score(y_test, y_pred))\n",
    "    final_recall_all.append(recall_score(y_test, y_pred))\n",
    "    final_f1_all.append(f1_score(y_test, y_pred))\n",
    "    # Now load the best weights and predict on test data again\n",
    "    if \"cp\" in callback:\n",
    "            model.load_weights(model_cp_filepath)\n",
    "            best_pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "            best_y_pred = np.round(best_pred_test)\n",
    "            best_bal_acc_all.append(balanced_accuracy_score(y_test, best_y_pred))\n",
    "            best_precision_all.append(precision_score(y_test, best_y_pred))\n",
    "            best_recall_all.append(recall_score(y_test, best_y_pred))\n",
    "            best_f1_all.append(f1_score(y_test, best_y_pred))\n",
    "\n",
    "# Determine best fold and predict on holdout set\n",
    "best_fold = np.argmax(best_f1_all)\n",
    "# best_fold_filepath = f'zhenhao_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "best_fold_filepath = f'hybrid_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "model.load_weights(best_fold_filepath)\n",
    "pred_holdout= model.predict(X_holdout_dict, batch_size=batch_size)\n",
    "y_pred_holdout = np.round(pred_holdout)\n",
    "\n",
    "end = time.time()\n",
    "execution_time = int(end - start)\n",
    "\n",
    "scores = [\n",
    "    run_name,\n",
    "    time.ctime(),\n",
    "    sampling_strategy,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    batch_size,\n",
    "    trainable,\n",
    "    dropout,\n",
    "    val_split,\n",
    "    callback,\n",
    "    callback_monitor,\n",
    "    num_nodes,\n",
    "    num_epochs,\n",
    "    class_weight,\n",
    "    list(map(lambda x: x.name if callable(x) else x, cmpltn_metrics)),\n",
    "    run_folder,\n",
    "    execution_time,\n",
    "    f\"{np.mean(final_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_precision_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_recall_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_f1_all, axis=0):.3f}\"[2:],\n",
    "    f\"{np.mean(best_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_precision_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_recall_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_f1_all, axis=0):.3f}\"[2:],\n",
    "    best_fold,\n",
    "    f\"{best_f1_all[best_fold]:.3f}\"[2:],\n",
    "    f\"{balanced_accuracy_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{precision_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{recall_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{f1_score(y_holdout, y_pred_holdout):.3f}\"[2:],\n",
    "]\n",
    "out = open(\"results.txt\", \"a\")\n",
    "# out.write(iteration_features + \", Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1, Best_Fold, Best_Fold_F1, Best_Fold_Holdout_Bal_Acc, Best_Fold_Holdout_Prec, Best_Fold_Holdout_Recall, Best_Fold_Holdout_F1 \\n\")\n",
    "out.write(str(scores).replace(\"'\", \"\")[1:-1] + \"\\n\")\n",
    "out.close()\n",
    "\n",
    "prog_log = open(\"progess.log\", \"a\")\n",
    "prog_log.write(f\"{time.ctime()} Finished fold {n_splits}. Run folder: {run_folder}\\n\")\n",
    "prog_log.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a model that did well\n",
    "# checkpoint_filepath = 'zhenhao_models/174repos_min50_max1000000/4609183334028858880/fold2'\n",
    "# model.load_weights(checkpoint_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test on the last fold's test set (bad)\n",
    "# pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "# y_pred = np.round(pred_test)\n",
    "# print(balanced_accuracy_score(y_test, y_pred))\n",
    "# print(precision_score(y_test, y_pred))\n",
    "# print(recall_score(y_test, y_pred))\n",
    "# print(f1_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the weights of the best fold\n",
    "# checkpoint_filepath = f'zhenhao_models/{repo_name}/{settings_hash}/fold{best_fold}'\n",
    "# checkpoint_filepath = f'hybrid_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "# Alternatively, Load the weights of a model that did well\n",
    "# checkpoint_filepath = 'zhenhao_models/174repos_min50_max1000000/4609183334028858880/fold2'\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# Test on the holdout set (good)\n",
    "# pred_holdout= model.predict(X_holdout_dict, batch_size=batch_size)\n",
    "# y_pred_holdout = np.round(pred_holdout)\n",
    "# print(balanced_accuracy_score(y_holdout, y_pred_holdout))\n",
    "# print(precision_score(y_holdout, y_pred_holdout))\n",
    "# print(recall_score(y_holdout, y_pred_holdout))\n",
    "# print(f1_score(y_holdout, y_pred_holdout))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Legacy only \"others\" run\n",
    "legacy = False\n",
    "if legacy:\n",
    "    run_name = f\"{repo_name[:3]}_min{min_val}_onlyOthers_new_keyword_cv{n_splits}\"\n",
    "    settings_hash = abs(hash(str(time.ctime())))\n",
    "\n",
    "    # DEBUG\n",
    "    debug = False\n",
    "    if debug:\n",
    "        num_epochs = 1\n",
    "        batch_size = 256\n",
    "        n_splits = 1\n",
    "        settings_hash = int((hash(str(time.ctime())) ** 2) ** 0.5)\n",
    "        run_name = f\"DEBUG_{run_name}\"\n",
    "    # /DEBUG\n",
    "\n",
    "    start = time.time()\n",
    "    histories = []\n",
    "\n",
    "\n",
    "    # List of (X_test_dict, y_test) of all folds\n",
    "    test_sets = []\n",
    "\n",
    "    # N.y.i. on remote\n",
    "    # model = build_others_model(vocab_size, output_dims, embedding_matrix, max_length,\n",
    "    #                        trainable, num_nodes, dropout, X.shape[1] - 1)\n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(X.shape[1] - 1,)))\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=cmpltn_metrics)\n",
    "\n",
    "    final_bal_acc_all, final_precision_all, final_recall_all, final_f1_all = [], [], [], []\n",
    "    best_bal_acc_all, best_precision_all, best_recall_all, best_f1_all = [], [], [], []\n",
    "    # K-fold cross-validation\n",
    "    if n_splits == 1:\n",
    "        indices = np.arange(y.shape[0])\n",
    "        strat_train_idx, strat_val_idx = train_test_split(indices, test_size=0.25, stratify=y, random_state=0)\n",
    "        idx_iter = [(strat_train_idx, strat_val_idx)]\n",
    "    else:\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=0)\n",
    "        idx_iter = skf.split(X, y)\n",
    "    for k_fold, (train_index, test_index) in enumerate(idx_iter):\n",
    "        print(f\"Starting fold {k_fold + 1} of {n_splits}.\")\n",
    "        prog_log = open(\"progess.log\", \"a\")\n",
    "        prog_log.write(f\"{time.ctime()} Starting fold {k_fold + 1} of {n_splits}. Settings hash: {settings_hash}\\n\")\n",
    "        prog_log.close()\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # Oversample the minority class\n",
    "        sampler = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "        # Pad the context to create the context input\n",
    "        # padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "        # padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "        # Prepare the \"other\" input\n",
    "        regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "        regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "        # Put both inputs into a dict\n",
    "        # X_train_dict = {\"context\": padded_inputs, \"other\": regular_inputs}\n",
    "        # X_test_dict = {\"context\": padded_inputs_test, \"other\": regular_inputs_test}\n",
    "        # Append to the list of test sets\n",
    "        # test_sets.append((X_test_dict, y_test))\n",
    "        # Build the callbacks\n",
    "        callbacks, model_cp_filepath = build_callbacks(callback, callback_monitor, repo_name, settings_hash, k_fold)\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            regular_inputs,\n",
    "            y_train,\n",
    "            epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(regular_inputs_test, y_test),\n",
    "            validation_split=val_split,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        histories.append(history)\n",
    "\n",
    "        # Predict with final weights\n",
    "        pred_test= model.predict(regular_inputs_test, batch_size=batch_size)\n",
    "        y_pred = np.round(pred_test)\n",
    "        final_bal_acc_all.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        final_precision_all.append(precision_score(y_test, y_pred))\n",
    "        final_recall_all.append(recall_score(y_test, y_pred))\n",
    "        final_f1_all.append(f1_score(y_test, y_pred))\n",
    "        # Now load the best weights and predict on test data again\n",
    "        if \"cp\" in callback:\n",
    "                model.load_weights(model_cp_filepath)\n",
    "                best_pred_test= model.predict(regular_inputs_test, batch_size=batch_size)\n",
    "                best_y_pred = np.round(best_pred_test)\n",
    "                best_bal_acc_all.append(balanced_accuracy_score(y_test, best_y_pred))\n",
    "                best_precision_all.append(precision_score(y_test, best_y_pred))\n",
    "                best_recall_all.append(recall_score(y_test, best_y_pred))\n",
    "                best_f1_all.append(f1_score(y_test, best_y_pred))\n",
    "\n",
    "    # Determine best fold and predict on holdout set\n",
    "    best_fold = np.argmax(best_f1_all)\n",
    "    best_fold_filepath = f'zhenhao_models/{repo_name}/{settings_hash}/fold{best_fold}'\n",
    "    model.load_weights(best_fold_filepath)\n",
    "    pred_holdout= model.predict(regular_inputs_holdout, batch_size=batch_size)\n",
    "    y_pred_holdout = np.round(pred_holdout)\n",
    "\n",
    "    end = time.time()\n",
    "    execution_time = int(end - start)\n",
    "\n",
    "    scores = [\n",
    "        run_name,\n",
    "        time.ctime(),\n",
    "        sampling_strategy,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        trainable,\n",
    "        dropout,\n",
    "        val_split,\n",
    "        callback,\n",
    "        callback_monitor,\n",
    "        num_nodes,\n",
    "        num_epochs,\n",
    "        class_weight,\n",
    "        list(map(lambda x: x.name if callable(x) else x, cmpltn_metrics)),\n",
    "        settings_hash,\n",
    "        execution_time,\n",
    "        f\"{np.mean(final_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_f1_all, axis=0):.3f}\"[2:],\n",
    "        f\"{np.mean(best_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_f1_all, axis=0):.3f}\"[2:],\n",
    "        best_fold,\n",
    "        f\"{best_f1_all[best_fold]:.3f}\"[2:],\n",
    "        f\"{balanced_accuracy_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{precision_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{recall_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{f1_score(y_holdout, y_pred_holdout):.3f}\"[2:],\n",
    "    ]\n",
    "    out = open(\"results.txt\", \"a\")\n",
    "    # out.write(iteration_features + \", Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1, Best_Fold, Best_Fold_F1, Best_Fold_Holdout_Bal_Acc, Best_Fold_Holdout_Prec, Best_Fold_Holdout_Recall, Best_Fold_Holdout_F1 \\n\")\n",
    "    out.write(str(scores).replace(\"'\", \"\")[1:-1] + \"\\n\")\n",
    "    out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}