{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from notebook_helper import MyCorpus, build_hybrid_model, build_callbacks, build_embedding_matrix, iteration_features\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from string import ascii_letters\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "# Keras specific\n",
    "\n",
    "#### CHANGED from import keras:\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "#####\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, CuDNNLSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# Gemsim\n",
    "import gensim.models\n",
    "from gensim import utils\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "\n",
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "run_random_forests = True\n",
    "cap_at_one = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Before:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    print(\"After:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "except IndexError as e:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter vecs:\t\t18293\n",
      "without logging (negatives):\t17987\n",
      "with logging (positives):\t\t306\n",
      "Log ratio:\t\t\t\t\t\t1.67%\n",
      "(18293, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  type      location  length parent                              context  \\\n0    c  105;0-128;23      24      a                cqqdBdBdqirrrrmqrerrr   \n1    c  133;4-138;29       6      d                              cderrrr   \n2    d    14;0-16;50       3      a                                 durr   \n3    d    18;0-60;18      43      a  deqrrerueruferuerqrrqrqrqrfrerukruu   \n4    d    63;0-67;51       5      a                                  drr   \n\n   contains_class_definition  contains_function_definition  \\\n0                          0                             3   \n1                          0                             1   \n2                          0                             0   \n3                          0                             0   \n4                          0                             0   \n\n   contains_if_statement  contains_for_statement  contains_match_statement  \\\n0                      0                       0                         0   \n1                      0                       0                         0   \n2                      0                       0                         0   \n3                      4                       2                         0   \n4                      0                       0                         0   \n\n   ...  contains_exec_statement  contains_future_import_statement  \\\n0  ...                        0                                 0   \n1  ...                        0                                 0   \n2  ...                        0                                 0   \n3  ...                        0                                 0   \n4  ...                        0                                 0   \n\n   contains_global_statement  contains_nonlocal_statement  \\\n0                          0                            0   \n1                          0                            0   \n2                          0                            0   \n3                          0                            0   \n4                          0                            0   \n\n   contains_print_statement  contains_assignment  contains_call  \\\n0                         0                    2              0   \n1                         0                    0              0   \n2                         0                    0              0   \n3                         0                    3              3   \n4                         0                    0              2   \n\n   contains_await  contains_yield  contains_logging  \n0               0               0                 0  \n1               0               0                 0  \n2               0               0                 0  \n3               0               0                 0  \n4               0               0                 0  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>location</th>\n      <th>length</th>\n      <th>parent</th>\n      <th>context</th>\n      <th>contains_class_definition</th>\n      <th>contains_function_definition</th>\n      <th>contains_if_statement</th>\n      <th>contains_for_statement</th>\n      <th>contains_match_statement</th>\n      <th>...</th>\n      <th>contains_exec_statement</th>\n      <th>contains_future_import_statement</th>\n      <th>contains_global_statement</th>\n      <th>contains_nonlocal_statement</th>\n      <th>contains_print_statement</th>\n      <th>contains_assignment</th>\n      <th>contains_call</th>\n      <th>contains_await</th>\n      <th>contains_yield</th>\n      <th>contains_logging</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c</td>\n      <td>105;0-128;23</td>\n      <td>24</td>\n      <td>a</td>\n      <td>cqqdBdBdqirrrrmqrerrr</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c</td>\n      <td>133;4-138;29</td>\n      <td>6</td>\n      <td>d</td>\n      <td>cderrrr</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d</td>\n      <td>14;0-16;50</td>\n      <td>3</td>\n      <td>a</td>\n      <td>durr</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d</td>\n      <td>18;0-60;18</td>\n      <td>43</td>\n      <td>a</td>\n      <td>deqrrerueruferuerqrrqrqrqrfrerukruu</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d</td>\n      <td>63;0-67;51</td>\n      <td>5</td>\n      <td>a</td>\n      <td>drr</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "min_val = 50\n",
    "repo_name = f\"174repos_min{min_val}_max1000000\"\n",
    "# repo_name = f\"300repos_min{min_val}_max1000000\"\n",
    "# repo_name = f\"combination\"\n",
    "# df = pd.read_csv('../features/'+ repo_name +'_new_keyword_expanded.csv')\n",
    "# df = pd.read_csv('../features/'+ repo_name +'_counts_expanded.csv')\n",
    "df = pd.read_csv('../features/'+ repo_name +'_node_len_expanded.csv')\n",
    "# df = pd.read_csv('../features/web2py.csv')\n",
    "\n",
    "# Remove errors\n",
    "df = df[df.parent != 'b']\n",
    "df = df[df.type != 'b']\n",
    "\n",
    "no_log_cnt, log_cnt = df['contains_logging'].value_counts()\n",
    "par_vec_cnt = no_log_cnt + log_cnt\n",
    "log_ratio = log_cnt / par_vec_cnt\n",
    "print(f\"Number of parameter vecs:\\t\\t{par_vec_cnt}\")\n",
    "print(f\"without logging (negatives):\\t{no_log_cnt}\")\n",
    "print(f\"with logging (positives):\\t\\t{log_cnt}\")\n",
    "print(f\"Log ratio:\\t\\t\\t\\t\\t\\t{log_ratio * 100:.2f}%\")\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if cap_at_one:\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"contains\"):\n",
    "            df[col] = df[col].apply(lambda x: min(1, x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  type      location  length parent  \\\n0    c  105;0-128;23      24      a   \n1    c  133;4-138;29       6      d   \n2    d    14;0-16;50       3      a   \n3    d    18;0-60;18      43      a   \n4    d    63;0-67;51       5      a   \n\n                                             context  \\\n0  [2, 16, 16, 3, 27, 3, 27, 3, 16, 8, 17, 17, 17...   \n1                          [2, 3, 4, 17, 17, 17, 17]   \n2                                    [3, 20, 17, 17]   \n3  [3, 4, 16, 17, 17, 4, 17, 20, 4, 17, 20, 5, 4,...   \n4                                        [3, 17, 17]   \n\n   contains_class_definition  contains_function_definition  \\\n0                          0                             3   \n1                          0                             1   \n2                          0                             0   \n3                          0                             0   \n4                          0                             0   \n\n   contains_if_statement  contains_for_statement  contains_match_statement  \\\n0                      0                       0                         0   \n1                      0                       0                         0   \n2                      0                       0                         0   \n3                      4                       2                         0   \n4                      0                       0                         0   \n\n   ...  contains_exec_statement  contains_future_import_statement  \\\n0  ...                        0                                 0   \n1  ...                        0                                 0   \n2  ...                        0                                 0   \n3  ...                        0                                 0   \n4  ...                        0                                 0   \n\n   contains_global_statement  contains_nonlocal_statement  \\\n0                          0                            0   \n1                          0                            0   \n2                          0                            0   \n3                          0                            0   \n4                          0                            0   \n\n   contains_print_statement  contains_assignment  contains_call  \\\n0                         0                    2              0   \n1                         0                    0              0   \n2                         0                    0              0   \n3                         0                    3              3   \n4                         0                    0              2   \n\n   contains_await  contains_yield  contains_logging  \n0               0               0                 0  \n1               0               0                 0  \n2               0               0                 0  \n3               0               0                 0  \n4               0               0                 0  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>location</th>\n      <th>length</th>\n      <th>parent</th>\n      <th>context</th>\n      <th>contains_class_definition</th>\n      <th>contains_function_definition</th>\n      <th>contains_if_statement</th>\n      <th>contains_for_statement</th>\n      <th>contains_match_statement</th>\n      <th>...</th>\n      <th>contains_exec_statement</th>\n      <th>contains_future_import_statement</th>\n      <th>contains_global_statement</th>\n      <th>contains_nonlocal_statement</th>\n      <th>contains_print_statement</th>\n      <th>contains_assignment</th>\n      <th>contains_call</th>\n      <th>contains_await</th>\n      <th>contains_yield</th>\n      <th>contains_logging</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c</td>\n      <td>105;0-128;23</td>\n      <td>24</td>\n      <td>a</td>\n      <td>[2, 16, 16, 3, 27, 3, 27, 3, 16, 8, 17, 17, 17...</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c</td>\n      <td>133;4-138;29</td>\n      <td>6</td>\n      <td>d</td>\n      <td>[2, 3, 4, 17, 17, 17, 17]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d</td>\n      <td>14;0-16;50</td>\n      <td>3</td>\n      <td>a</td>\n      <td>[3, 20, 17, 17]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d</td>\n      <td>18;0-60;18</td>\n      <td>43</td>\n      <td>a</td>\n      <td>[3, 4, 16, 17, 17, 4, 17, 20, 4, 17, 20, 5, 4,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d</td>\n      <td>63;0-67;51</td>\n      <td>5</td>\n      <td>a</td>\n      <td>[3, 17, 17]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the compacted context from letters into strings of integers\n",
    "df.context = [list(map(lambda y: str(ascii_letters.index(y)), list(str(x)))) for x in df.context]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df.drop([\"location\", \"contains_logging\"], axis=1)\n",
    "# X = df.drop([\"contains_logging\"], axis=1)\n",
    "X = pd.get_dummies(X, columns=[\"type\", \"parent\"])\n",
    "y = df.contains_logging\n",
    "# Keep holdout set for testing after k-fold cross validation\n",
    "X, X_holdout, y, y_holdout = train_test_split(X, y, test_size = 0.1, stratify=y, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec Model\n",
    "\n",
    "sentences = MyCorpus(list(df.context))\n",
    "gensim_model = gensim.models.Word2Vec(sentences=sentences, min_count=1)\n",
    "actual_vocab_size = len(gensim_model.wv.key_to_index)\n",
    "actual_vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build an embedding for each context as the mean of its words' Word2Vec vectors\n",
    "# corpus_embeddings = []\n",
    "# for doc in sentences:\n",
    "#     doc_embedding = np.zeros((len(doc), 100), dtype=np.float32)\n",
    "#     for idx, word in enumerate(doc):\n",
    "#                     doc_embedding[idx] = gensim_model.wv[word]\n",
    "#     doc_embedding = np.mean(doc_embedding, axis=0)\n",
    "#     corpus_embeddings.append(doc_embedding)\n",
    "# corpus_embeddings = np.array(corpus_embeddings)\n",
    "# df_corpus_embeddings = pd.DataFrame(corpus_embeddings, columns=[\"c\"+str(i) for i in range(100)])\n",
    "# df_corpus_embeddings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# vectorizer = TfidfVectorizer(token_pattern=\"\\w+\")\n",
    "# concat_context = [\" \".join(x) for x in df.context]\n",
    "# vectorize_res = vectorizer.fit_transform(concat_context)\n",
    "# df_tfidf = pd.DataFrame(vectorize_res.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# df_tfidf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "score_names = [\n",
    "    \"Balanced accuracy score\",\n",
    "    \"Precision score\",\n",
    "    \"Recall score\",\n",
    "    \"F1 Binary\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "'174repos_min50_max1000000_node_len'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_sampling = False\n",
    "rnd_repo_name = repo_name + (\"_counts\" if not cap_at_one else \"\") + \"_node_len\" + (\"_over\" if use_sampling else \"\")\n",
    "rnd_repo_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 4\n",
      "Starting fold 1.\n",
      "Starting fold 2.\n",
      "Starting fold 3.\n",
      "Starting fold 4.\n",
      "Starting fold 5.\n",
      "Balanced accuracy score    0.693\n",
      "Precision score            0.543\n",
      "Recall score               0.391\n",
      "F1 Binary                  0.454\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folded Random Forest Run\n",
    "if run_random_forests:\n",
    "    X_rnd = X.drop([\"context\"], axis=1)\n",
    "    rnd_forest_sampling_strategy = 0.05\n",
    "    use_sampling = False\n",
    "    rnd_repo_name = repo_name + (\"_counts\" if not cap_at_one else \"\") + \"_node_len\" + (\"_over\" if use_sampling else \"\")\n",
    "    n_splits=5\n",
    "    # Hyper Params\n",
    "    n_estimators = 9\n",
    "    min_samples_split = 5\n",
    "    min_samples_leaf = 1\n",
    "    max_depth = None\n",
    "    class_weight = {False: 1, True: 4}\n",
    "\n",
    "    # List of the k classifiers from the k-fold split\n",
    "    # classifiers = []\n",
    "\n",
    "    df_regular = df.drop([\"context\"], axis=1)\n",
    "    dataframes = [\n",
    "        (\"regular\", df_regular),\n",
    "        # (\"corpus\", df_corpus_embeddings.join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"tf-idf\", df_tfidf.join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"regular+corpus\", df_regular.join(df_corpus_embeddings)),\n",
    "        # (\"regular+tfidf\", df_regular.join(df_tfidf)),\n",
    "        # (\"corpus+tfidf\", df_corpus_embeddings.join(df_tfidf).join(pd.DataFrame(df.contains_logging))),\n",
    "        # (\"regular+corpus+tfidf\", df_regular.join(df_corpus_embeddings).join(df_tfidf)),\n",
    "    ]\n",
    "\n",
    "    true_weight = 4\n",
    "    # for df_name, df_used in dataframes:\n",
    "    #     print(df_name)\n",
    "    # for true_weight in [2,3,3.5,4,4.5,5]:\n",
    "    for true_weight in [4]:\n",
    "    # for n_estimators in [31,50,100]:\n",
    "        class_weight = {False: 1, True: true_weight}\n",
    "        print(f\"Weight: {class_weight[True]} Estimators: {n_estimators}\")\n",
    "        all_scores = []\n",
    "        conf_matrices = []\n",
    "        # Split data into train and test sets\n",
    "        # X = df_used.drop([\"contains_logging\", \"location\"], axis=1)\n",
    "        # if \"regular\" in df_name:\n",
    "        #     X = pd.get_dummies(X, columns=[\"type\", \"parent\"])\n",
    "        # y = df_used.contains_logging\n",
    "\n",
    "        # classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "        # classifier = RandomForestClassifier(n_estimators=9, random_state=0)\n",
    "        # skf = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25)\n",
    "        for k_fold, (train_index, test_index) in enumerate(skf.split(X_rnd, y)):\n",
    "            print(f\"Starting fold {k_fold + 1}.\")\n",
    "            classifier = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           max_depth=max_depth,\n",
    "                                           class_weight=class_weight)\n",
    "            X_train, X_test = X_rnd.iloc[train_index], X_rnd.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            # Sampling\n",
    "            if use_sampling:\n",
    "                # sampler = RandomUnderSampler(sampling_strategy=rnd_forest_sampling_strategy)\n",
    "                sampler = RandomOverSampler(sampling_strategy=rnd_forest_sampling_strategy)\n",
    "                X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            classifiers.append(classifier)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            scores = [\n",
    "                    balanced_accuracy_score(y_test, y_pred),\n",
    "                    precision_score(y_test, y_pred),\n",
    "                    recall_score(y_test, y_pred),\n",
    "                    f1_score(y_test, y_pred, average='binary', pos_label=True)\n",
    "            ]\n",
    "            all_scores.append(scores)\n",
    "            cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)\n",
    "            conf_matrices.append(cm)\n",
    "        score_df = pd.DataFrame(all_scores, columns=score_names).mean().round(3)\n",
    "\n",
    "        # Testing with the holdout set, slightly different results but ultimately useless\n",
    "        # X_rnd_holdout = X_holdout.drop([\"context\"], axis=1)\n",
    "        # holdout_scores = []\n",
    "        # for clf in classifiers:\n",
    "        #     y_pred_holdout = clf.predict(X_rnd_holdout)\n",
    "        #     scores = [\n",
    "        #         balanced_accuracy_score(y_holdout, y_pred_holdout),\n",
    "        #         precision_score(y_holdout, y_pred_holdout),\n",
    "        #         recall_score(y_holdout, y_pred_holdout),\n",
    "        #         f1_score(y_holdout, y_pred_holdout, average='binary', pos_label=True)\n",
    "        #     ]\n",
    "        #     holdout_scores.append(scores)\n",
    "\n",
    "        print(score_df)\n",
    "        avg_cm = np.mean(conf_matrices, axis=0).astype(int)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=avg_cm,\n",
    "                                      display_labels=classifier.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.show()\n",
    "        out = open(\"my_approach_rnd_forest_results\", \"a\")\n",
    "        # out.write(\"Name, Timestamp, use_sampling, sampling_strategy, n_estimators, min_samples_split, class_weight, \" + \", \".join(score_names) + \"\\n\")\n",
    "        out.write(f\"{rnd_repo_name} {time.ctime()}, {use_sampling}, {rnd_forest_sampling_strategy}, {n_estimators}, {min_samples_split}, {class_weight}, \" +\n",
    "                  \", \".join([str(x) for x in score_df.values]) + \"\\n\")\n",
    "        out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest: Feature importance based on mean decrease in impurity\n",
    "if run_random_forests:\n",
    "    sort_importances = True\n",
    "\n",
    "    importances = classifier.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in classifier.estimators_], axis=0)\n",
    "    if sort_importances:\n",
    "        importances,std,columns = list(zip(*sorted(list(zip(importances, std, X.columns)), reverse=True)))\n",
    "    else:\n",
    "        columns = X.columns\n",
    "\n",
    "    forest_importances = pd.Series(importances, index=columns)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.set_size_inches(18.5, 18.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest: Feature importance based on feature permutation\n",
    "if run_random_forests and False:\n",
    "    # result = permutation_importance(classifier, X_rnd, y, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "    result = permutation_importance(classifier, X_holdout.drop([\"context\"], axis=1), y_holdout, n_repeats=10, n_jobs=-1)\n",
    "    forest_permutation_importances = pd.Series(result.importances_mean, index=X_rnd.columns)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_permutation_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "    ax.set_title(\"Feature importances using permutation on full model\")\n",
    "    ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(18.5, 18.5)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data split for Tensorflow\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=0)\n",
    "# X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"Negatives: \" + str(y_train[y_train == 0].shape[0]))\n",
    "# print(\"Positives: \" + str(y_train[y_train == 1].shape[0]))\n",
    "# print(\"Ratio:\\t   \" + f\"{y_train[y_train == 1].shape[0] / y_train[y_train == 0].shape[0] * 100:.2f}\" + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Smote doesn't seem to work with the context column\n",
    "# over = SMOTE(random_state=0, sampling_strategy=0.1)\n",
    "# X_train_resampled, y_train_resampled = over.fit_resample(X_train, y_train)\n",
    "\n",
    "# over = RandomOverSampler(random_state=0, sampling_strategy=0.1)\n",
    "# X_train_resampled, y_train_resampled = over.fit_resample(X_train, y_train)\n",
    "\n",
    "# sampling_strategy = 0.05\n",
    "# Choose one:\n",
    "# sampler = RandomUnderSampler(random_state=0, sampling_strategy=sampling_strategy)\n",
    "# sampler = RandomOverSampler(random_state=0, sampling_strategy=sampling_strategy)\n",
    "\n",
    "# X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"Negatives: \" + str(y_train_resampled[y_train_resampled == 0].shape[0]))\n",
    "# print(\"Positives: \" + str(y_train_resampled[y_train_resampled == 1].shape[0]))\n",
    "# print(\"Ratio:\\t   \" + f\"{y_train_resampled[y_train_resampled == 1].shape[0] / y_train_resampled[y_train_resampled == 0].shape[0] * 100:.2f}\" + \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_train, y_train = X_train_resampled, y_train_resampled"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sampling_strategy = 0.05\n",
    "sampling_strategy = 0.05\n",
    "vocab_size = actual_vocab_size + 1\n",
    "output_dims = 100\n",
    "max_length = 80\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "trainable=True\n",
    "dropout = 0.2\n",
    "val_split = 0.0\n",
    "num_nodes = 128\n",
    "callback = [\"cp\"]\n",
    "callback_monitor = 'val_f1_score'\n",
    "class_weight = \"class_weight_unsupported\"\n",
    "cmpltn_metrics = [tfa.metrics.F1Score(num_classes=1, threshold=0.5)]\n",
    "\n",
    "# Cross-validation settings\n",
    "n_splits = 3\n",
    "\n",
    "settings_hash = abs(hash(str([n_splits, sampling_strategy, vocab_size, output_dims, max_length, num_epochs, batch_size, trainable, dropout, val_split, num_nodes, callback, callback_monitor, class_weight, cmpltn_metrics]))) # TODO: DEPRECATE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = build_embedding_matrix(vocab_size, output_dims, gensim_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DEPRECATED: Not usable with k-fold cross-validation\n",
    "# Pad the context\n",
    "# padded_context = pad_sequences(np.array(X.context), maxlen=max_length, value=0.0)\n",
    "# Prepare the \"other\" input\n",
    "# other_input = X.drop([\"context\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# Pad the context to create the context input\n",
    "# padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "# padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "# padded_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# Prepare the \"other\" input\n",
    "# regular_inputs = X_train.drop([\"context\", \"location\"], axis=1)\n",
    "# regular_inputs_test = X_test.drop([\"context\", \"location\"], axis=1)\n",
    "# regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "# regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "# regular_inputs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare holdout test sets\n",
    "padded_inputs_holdout = pad_sequences(np.array(list(X_holdout.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "regular_inputs_holdout = X_holdout.drop([\"context\"], axis=1)\n",
    "X_holdout_dict = {\"context\": padded_inputs_holdout, \"other\": regular_inputs_holdout}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_name = f\"{repo_name[:3]}_min{min_val}_hybrid\" + (\"_counts\" if not cap_at_one else \"\") + f\"_node_len_cv{n_splits}\"\n",
    "run_number = 1\n",
    "# if not os.path.isdir(f\"hybrid_models\"):\n",
    "if not os.getcwd().endswith(\"notebooks\"):\n",
    "    raise Exception(\"Bad working directory\")\n",
    "while os.path.isdir(f\"hybrid_models/{repo_name}/run{run_number}/\"):\n",
    "    run_number += 1\n",
    "run_folder = f\"run{run_number}\"\n",
    "\n",
    "# DEBUG\n",
    "debug = False\n",
    "if debug:\n",
    "    num_epochs = 1\n",
    "    batch_size = 256\n",
    "    n_splits = 1\n",
    "    settings_hash = abs(hash(str(time.ctime())))\n",
    "    run_folder = settings_hash\n",
    "    run_name = f\"DEBUG_{run_name}\"\n",
    "# /DEBUG\n",
    "\n",
    "start = time.time()\n",
    "histories = []\n",
    "\n",
    "# List of (X_test_dict, y_test) of all folds\n",
    "test_sets = []\n",
    "\n",
    "model = build_hybrid_model(vocab_size, output_dims, embedding_matrix, max_length,\n",
    "                       trainable, num_nodes, dropout, X.shape[1] - 1)\n",
    "# model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=cmpltn_metrics)\n",
    "\n",
    "final_bal_acc_all, final_precision_all, final_recall_all, final_f1_all = [], [], [], []\n",
    "best_bal_acc_all, best_precision_all, best_recall_all, best_f1_all = [], [], [], []\n",
    "# K-fold cross-validation\n",
    "if n_splits == 1:\n",
    "    indices = np.arange(y.shape[0])\n",
    "    strat_train_idx, strat_val_idx = train_test_split(indices, test_size=0.25, stratify=y, random_state=0)\n",
    "    idx_iter = [(strat_train_idx, strat_val_idx)]\n",
    "else:\n",
    "    skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=0)\n",
    "    idx_iter = skf.split(X, y)\n",
    "for k_fold, (train_index, test_index) in enumerate(idx_iter):\n",
    "    print(f\"Starting fold {k_fold + 1} of {n_splits}.\")\n",
    "    prog_log = open(\"progess.log\", \"a\")\n",
    "    prog_log.write(f\"{time.ctime()} Starting fold {k_fold + 1} of {n_splits}. Run folder: {run_folder}\\n\")\n",
    "    prog_log.close()\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # Oversample the minority class\n",
    "    sampler = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    # Pad the context to create the context input\n",
    "    padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "    padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "    # Prepare the \"other\" input\n",
    "    regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "    regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "    # Put both inputs into a dict\n",
    "    X_train_dict = {\"context\": padded_inputs, \"other\": regular_inputs}\n",
    "    X_test_dict = {\"context\": padded_inputs_test, \"other\": regular_inputs_test}\n",
    "    # Append to the list of test sets\n",
    "    test_sets.append((X_test_dict, y_test))\n",
    "    # Build the callbacks\n",
    "    callbacks, model_cp_filepath = build_callbacks(callback, callback_monitor, repo_name, run_folder, k_fold, zhenhao=False)\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train_dict,\n",
    "        {\"logging\": y_train},\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_dict, y_test),\n",
    "        validation_split=val_split,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    histories.append(history)\n",
    "\n",
    "    # Predict with final weights\n",
    "    pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "    y_pred = np.round(pred_test)\n",
    "    final_bal_acc_all.append(balanced_accuracy_score(y_test, y_pred))\n",
    "    final_precision_all.append(precision_score(y_test, y_pred))\n",
    "    final_recall_all.append(recall_score(y_test, y_pred))\n",
    "    final_f1_all.append(f1_score(y_test, y_pred))\n",
    "    # Now load the best weights and predict on test data again\n",
    "    if \"cp\" in callback:\n",
    "            model.load_weights(model_cp_filepath)\n",
    "            best_pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "            best_y_pred = np.round(best_pred_test)\n",
    "            best_bal_acc_all.append(balanced_accuracy_score(y_test, best_y_pred))\n",
    "            best_precision_all.append(precision_score(y_test, best_y_pred))\n",
    "            best_recall_all.append(recall_score(y_test, best_y_pred))\n",
    "            best_f1_all.append(f1_score(y_test, best_y_pred))\n",
    "\n",
    "# Determine best fold and predict on holdout set\n",
    "best_fold = np.argmax(best_f1_all)\n",
    "# best_fold_filepath = f'zhenhao_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "best_fold_filepath = f'hybrid_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "model.load_weights(best_fold_filepath)\n",
    "pred_holdout= model.predict(X_holdout_dict, batch_size=batch_size)\n",
    "y_pred_holdout = np.round(pred_holdout)\n",
    "\n",
    "end = time.time()\n",
    "execution_time = int(end - start)\n",
    "\n",
    "scores = [\n",
    "    run_name,\n",
    "    time.ctime(),\n",
    "    sampling_strategy,\n",
    "    max_length,\n",
    "    vocab_size,\n",
    "    batch_size,\n",
    "    trainable,\n",
    "    dropout,\n",
    "    val_split,\n",
    "    callback,\n",
    "    callback_monitor,\n",
    "    num_nodes,\n",
    "    num_epochs,\n",
    "    class_weight,\n",
    "    list(map(lambda x: x.name if callable(x) else x, cmpltn_metrics)),\n",
    "    run_folder,\n",
    "    execution_time,\n",
    "    f\"{np.mean(final_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_precision_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_recall_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(final_f1_all, axis=0):.3f}\"[2:],\n",
    "    f\"{np.mean(best_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_precision_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_recall_all, axis=0):.2f}\"[2:],\n",
    "    f\"{np.mean(best_f1_all, axis=0):.3f}\"[2:],\n",
    "    best_fold,\n",
    "    f\"{best_f1_all[best_fold]:.3f}\"[2:],\n",
    "    f\"{balanced_accuracy_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{precision_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{recall_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "    f\"{f1_score(y_holdout, y_pred_holdout):.3f}\"[2:],\n",
    "]\n",
    "out = open(\"results.txt\", \"a\")\n",
    "# out.write(iteration_features + \", Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1, Best_Fold, Best_Fold_F1, Best_Fold_Holdout_Bal_Acc, Best_Fold_Holdout_Prec, Best_Fold_Holdout_Recall, Best_Fold_Holdout_F1 \\n\")\n",
    "out.write(str(scores).replace(\"'\", \"\")[1:-1] + \"\\n\")\n",
    "out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a model that did well\n",
    "# checkpoint_filepath = 'zhenhao_models/174repos_min50_max1000000/4609183334028858880/fold2'\n",
    "# model.load_weights(checkpoint_filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test on the last fold's test set (bad)\n",
    "# pred_test= model.predict(X_test_dict, batch_size=batch_size)\n",
    "# y_pred = np.round(pred_test)\n",
    "# print(balanced_accuracy_score(y_test, y_pred))\n",
    "# print(precision_score(y_test, y_pred))\n",
    "# print(recall_score(y_test, y_pred))\n",
    "# print(f1_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the weights of the best fold\n",
    "# checkpoint_filepath = f'zhenhao_models/{repo_name}/{settings_hash}/fold{best_fold}'\n",
    "# checkpoint_filepath = f'hybrid_models/{repo_name}/{run_folder}/fold{best_fold}'\n",
    "# Alternatively, Load the weights of a model that did well\n",
    "# checkpoint_filepath = 'zhenhao_models/174repos_min50_max1000000/4609183334028858880/fold2'\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# Test on the holdout set (good)\n",
    "# pred_holdout= model.predict(X_holdout_dict, batch_size=batch_size)\n",
    "# y_pred_holdout = np.round(pred_holdout)\n",
    "# print(balanced_accuracy_score(y_holdout, y_pred_holdout))\n",
    "# print(precision_score(y_holdout, y_pred_holdout))\n",
    "# print(recall_score(y_holdout, y_pred_holdout))\n",
    "# print(f1_score(y_holdout, y_pred_holdout))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Legacy only \"others\" run\n",
    "legacy = False\n",
    "if legacy:\n",
    "    run_name = f\"{repo_name[:3]}_min{min_val}_onlyOthers_new_keyword_cv{n_splits}\"\n",
    "    settings_hash = abs(hash(str(time.ctime())))\n",
    "\n",
    "    # DEBUG\n",
    "    debug = False\n",
    "    if debug:\n",
    "        num_epochs = 1\n",
    "        batch_size = 256\n",
    "        n_splits = 1\n",
    "        settings_hash = int((hash(str(time.ctime())) ** 2) ** 0.5)\n",
    "        run_name = f\"DEBUG_{run_name}\"\n",
    "    # /DEBUG\n",
    "\n",
    "    start = time.time()\n",
    "    histories = []\n",
    "\n",
    "\n",
    "    # List of (X_test_dict, y_test) of all folds\n",
    "    test_sets = []\n",
    "\n",
    "    # N.y.i. on remote\n",
    "    # model = build_others_model(vocab_size, output_dims, embedding_matrix, max_length,\n",
    "    #                        trainable, num_nodes, dropout, X.shape[1] - 1)\n",
    "    model = Sequential()\n",
    "    model.add(keras.Input(shape=(X.shape[1] - 1,)))\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=cmpltn_metrics)\n",
    "\n",
    "    final_bal_acc_all, final_precision_all, final_recall_all, final_f1_all = [], [], [], []\n",
    "    best_bal_acc_all, best_precision_all, best_recall_all, best_f1_all = [], [], [], []\n",
    "    # K-fold cross-validation\n",
    "    if n_splits == 1:\n",
    "        indices = np.arange(y.shape[0])\n",
    "        strat_train_idx, strat_val_idx = train_test_split(indices, test_size=0.25, stratify=y, random_state=0)\n",
    "        idx_iter = [(strat_train_idx, strat_val_idx)]\n",
    "    else:\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=0)\n",
    "        idx_iter = skf.split(X, y)\n",
    "    for k_fold, (train_index, test_index) in enumerate(idx_iter):\n",
    "        print(f\"Starting fold {k_fold + 1} of {n_splits}.\")\n",
    "        prog_log = open(\"progess.log\", \"a\")\n",
    "        prog_log.write(f\"{time.ctime()} Starting fold {k_fold + 1} of {n_splits}. Settings hash: {settings_hash}\\n\")\n",
    "        prog_log.close()\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # Oversample the minority class\n",
    "        sampler = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "        # Pad the context to create the context input\n",
    "        # padded_inputs = pad_sequences(np.array(list(X_train.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "        # padded_inputs_test = pad_sequences(np.array(list(X_test.context), dtype=object), maxlen=max_length, value=0.0)\n",
    "        # Prepare the \"other\" input\n",
    "        regular_inputs = X_train.drop([\"context\"], axis=1)\n",
    "        regular_inputs_test = X_test.drop([\"context\"], axis=1)\n",
    "        # Put both inputs into a dict\n",
    "        # X_train_dict = {\"context\": padded_inputs, \"other\": regular_inputs}\n",
    "        # X_test_dict = {\"context\": padded_inputs_test, \"other\": regular_inputs_test}\n",
    "        # Append to the list of test sets\n",
    "        # test_sets.append((X_test_dict, y_test))\n",
    "        # Build the callbacks\n",
    "        callbacks, model_cp_filepath = build_callbacks(callback, callback_monitor, repo_name, settings_hash, k_fold)\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            regular_inputs,\n",
    "            y_train,\n",
    "            epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(regular_inputs_test, y_test),\n",
    "            validation_split=val_split,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        histories.append(history)\n",
    "\n",
    "        # Predict with final weights\n",
    "        pred_test= model.predict(regular_inputs_test, batch_size=batch_size)\n",
    "        y_pred = np.round(pred_test)\n",
    "        final_bal_acc_all.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        final_precision_all.append(precision_score(y_test, y_pred))\n",
    "        final_recall_all.append(recall_score(y_test, y_pred))\n",
    "        final_f1_all.append(f1_score(y_test, y_pred))\n",
    "        # Now load the best weights and predict on test data again\n",
    "        if \"cp\" in callback:\n",
    "                model.load_weights(model_cp_filepath)\n",
    "                best_pred_test= model.predict(regular_inputs_test, batch_size=batch_size)\n",
    "                best_y_pred = np.round(best_pred_test)\n",
    "                best_bal_acc_all.append(balanced_accuracy_score(y_test, best_y_pred))\n",
    "                best_precision_all.append(precision_score(y_test, best_y_pred))\n",
    "                best_recall_all.append(recall_score(y_test, best_y_pred))\n",
    "                best_f1_all.append(f1_score(y_test, best_y_pred))\n",
    "\n",
    "    # Determine best fold and predict on holdout set\n",
    "    best_fold = np.argmax(best_f1_all)\n",
    "    best_fold_filepath = f'zhenhao_models/{repo_name}/{settings_hash}/fold{best_fold}'\n",
    "    model.load_weights(best_fold_filepath)\n",
    "    pred_holdout= model.predict(regular_inputs_holdout, batch_size=batch_size)\n",
    "    y_pred_holdout = np.round(pred_holdout)\n",
    "\n",
    "    end = time.time()\n",
    "    execution_time = int(end - start)\n",
    "\n",
    "    scores = [\n",
    "        run_name,\n",
    "        time.ctime(),\n",
    "        sampling_strategy,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        trainable,\n",
    "        dropout,\n",
    "        val_split,\n",
    "        callback,\n",
    "        callback_monitor,\n",
    "        num_nodes,\n",
    "        num_epochs,\n",
    "        class_weight,\n",
    "        list(map(lambda x: x.name if callable(x) else x, cmpltn_metrics)),\n",
    "        settings_hash,\n",
    "        execution_time,\n",
    "        f\"{np.mean(final_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_f1_all, axis=0):.3f}\"[2:],\n",
    "        f\"{np.mean(best_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_f1_all, axis=0):.3f}\"[2:],\n",
    "        best_fold,\n",
    "        f\"{best_f1_all[best_fold]:.3f}\"[2:],\n",
    "        f\"{balanced_accuracy_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{precision_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{recall_score(y_holdout, y_pred_holdout):.2f}\"[2:],\n",
    "        f\"{f1_score(y_holdout, y_pred_holdout):.3f}\"[2:],\n",
    "    ]\n",
    "    out = open(\"results.txt\", \"a\")\n",
    "    # out.write(iteration_features + \", Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1, Best_Fold, Best_Fold_F1, Best_Fold_Holdout_Bal_Acc, Best_Fold_Holdout_Prec, Best_Fold_Holdout_Recall, Best_Fold_Holdout_F1 \\n\")\n",
    "    out.write(str(scores).replace(\"'\", \"\")[1:-1] + \"\\n\")\n",
    "    out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}