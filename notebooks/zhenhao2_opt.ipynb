{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import time\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "\n",
    "#### CHANGED from import keras:\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "#####\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, CuDNNLSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# Gemsim\n",
    "import gensim.models\n",
    "from gensim import utils\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "After:\n",
      " [PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "except IndexError as e:\n",
    "    pass\n",
    "print(\"After:\\n\" ,tf.config.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "        \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "        def __init__(self, text_list: list):\n",
    "            self.text_list = text_list\n",
    "\n",
    "        def __iter__(self):\n",
    "            for line in self.text_list:\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
    "\n",
    "### BAD\n",
    "\n",
    "### Define F1 measures: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        recall = TP / (Positives+K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter vecs:\t\t2328579\n",
      "without logging (negatives):\t2272090\n",
      "with logging (positives):\t\t56489\n",
      "Log ratio:\t\t\t\t\t\t2.43%\n",
      "(2328579, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>location</th>\n",
       "      <th>context</th>\n",
       "      <th>contains_logging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d</td>\n",
       "      <td>11;4-15;47</td>\n",
       "      <td>dqrqrqrrr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d</td>\n",
       "      <td>19;4-50;57</td>\n",
       "      <td>dqrqrqrrrqrrqrrqrrrqrrqrrqrrrqrrrqrrr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d</td>\n",
       "      <td>54;4-110;57</td>\n",
       "      <td>dqrqrqqrrqrqrrrrqrrrqrrrqrrrqrrrqrqrrrrqrrqrrq...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>114;4-123;37</td>\n",
       "      <td>dqrqrqqrrrr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>144;4-153;5</td>\n",
       "      <td>dqrqrqrrrr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type      location                                            context  \\\n",
       "0    d    11;4-15;47                                          dqrqrqrrr   \n",
       "1    d    19;4-50;57              dqrqrqrrrqrrqrrqrrrqrrqrrqrrrqrrrqrrr   \n",
       "2    d   54;4-110;57  dqrqrqqrrqrqrrrrqrrrqrrrqrrrqrrrqrqrrrrqrrqrrq...   \n",
       "3    d  114;4-123;37                                        dqrqrqqrrrr   \n",
       "4    d   144;4-153;5                                         dqrqrqrrrr   \n",
       "\n",
       "   contains_logging  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "min_val = 50\n",
    "repo_name = f\"174repos_min{min_val}_max1000000_zhenhao\"\n",
    "# repo_name = f\"300repos_min{min_val}_max1000000_zhenhao\"\n",
    "# repo_name = f\"combination_zhenhao\"\n",
    "df = pd.read_csv('../features/'+ repo_name +'.csv')\n",
    "\n",
    "# Remove errors\n",
    "df = df[df.type != 'b']\n",
    "\n",
    "no_log_cnt, log_cnt = df['contains_logging'].value_counts()\n",
    "par_vec_cnt = no_log_cnt + log_cnt\n",
    "log_ratio = log_cnt / par_vec_cnt\n",
    "print(f\"Number of parameter vecs:\\t\\t{par_vec_cnt}\")\n",
    "print(f\"without logging (negatives):\\t{no_log_cnt}\")\n",
    "print(f\"with logging (positives):\\t\\t{log_cnt}\")\n",
    "print(f\"Log ratio:\\t\\t\\t\\t\\t\\t{log_ratio * 100:.2f}%\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X = df.context\n",
    "X = [list(map(lambda y: str(ascii_letters.index(y)), list(x))) for x in X]\n",
    "y = df.contains_logging\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=0)\n",
    "\n",
    "# one hot encode outputs, no longer used\n",
    "# y_train_categorical = to_categorical(y_train)\n",
    "y_train_categorical = None\n",
    "# y_test_categorical = to_categorical(y_test)\n",
    "# count_classes = y_test_categorical.shape[1]\n",
    "# assert count_classes == 2\n",
    "\n",
    "# Word2Vec Model\n",
    "sentences = MyCorpus(X)\n",
    "gensim_model = gensim.models.Word2Vec(sentences=sentences, min_count=1)\n",
    "actual_vocab_size = len(gensim_model.wv.key_to_index)\n",
    "\n",
    "# Pad the context prep\n",
    "X_train_party = np.array([list(x) for x in X_train], dtype=object)\n",
    "X_test_party = np.array([list(x) for x in X_test], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = actual_vocab_size + 1\n",
    "output_dims = 100\n",
    "max_length = 80\n",
    "num_epochs = 60\n",
    "# class_weight = {False: 1, True: 4}\n",
    "class_weight = {0: 1.0, 1: 4.0}\n",
    "batch_size = 24\n",
    "dropout = 0.2\n",
    "trainable = False\n",
    "callback = None\n",
    "callback_monitor = 'loss' # 'accuracy'\n",
    "cmpltn_metrics = ['accuracy', custom_f1]\n",
    "\n",
    "num_nodes = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_features =  \"Name, max_length, vocab_size, batch_size, trainable, dropout, sigmoid_out, val_split, callback, callback_monitor, num_nodes, num_epochs, class_weight, cmpltn_metrics\"\n",
    "iterations = [\n",
    "# 1st run\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"loss\", 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     # Checkpoint manuell laden sonderfall\n",
    "#     (f'Z_{repo_name}_cploaded', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"loss\", 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}_cploaded', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"loss\", 128, 60, {0: 1.0, 1: 4.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    \n",
    "#     (f'Y_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "# 2nd Run loss vs acc\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"loss\", 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}_cploaded', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"loss\", 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}_cploaded', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "# 3rd Run other Layers, num_nodes, vocab_size\n",
    "#     (f'A_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'B_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    \n",
    "    # (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], tfa.metrics.F1Score, 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    # # val_loss is for validation data: https://keras.io/api/callbacks/model_checkpoint/\n",
    "    # (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], \"val_loss\", 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    \n",
    "    \n",
    "#     (f'D_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', actual_vocab_size, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', actual_vocab_size, 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'D_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', int(actual_vocab_size / 2), 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', int(actual_vocab_size / 2), 60, {0: 1.0, 1: 1.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', int(actual_vocab_size * 2), 60, {0: 1.0, 1: 1.0}),\n",
    "    \n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size, 24, True, 0.2, True, 0.0, [\"cp\"], 'loss', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    # (f'D_{repo_name}', 80, actual_vocab_size, 24, True, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "    \n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, False, 0.2, True, 0.0, [\"cp\"], 'loss', 128, 60, {0: 1.0, 1: 1.0}),\n",
    "\n",
    "# 4th run combined promising changes, checkpoint, later class_weight\n",
    "# Using layer set Z instead of B for more epochs in same execution time\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'custom_f1', 128, 2, {0: 1.0, 1: 4.0}, ['accuracy', custom_f1]),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 2, {0: 1.0, 1: 4.0}, ['accuracy']),\n",
    "    \n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 3.0}, ['unused']),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 4.0}, ['unused']),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 5.0}, ['unused']),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 6.0}, ['unused']),\n",
    "\n",
    "# Vary batch size\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 64, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 5.0}, ['unused']),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'f1_score', 128, 60, {0: 1.0, 1: 5.0}, ['unused']),\n",
    "\n",
    "\n",
    "# Test model.fit(validation_data=(padded_inputs_test, y_test)) and callback_monitor='val_f1_score'\n",
    "        (f'Z_{repo_name}', 80, actual_vocab_size + 1, 32, True, 0.2, True, 0.0, [\"cp\"], 'val_f1_score', 128, 20, {0: 1.0, 1: 5.0}, ['unused']),\n",
    "# TODO: Do Crossvalidation stratified shuffled fold testing with high batch size to compensate\n",
    "\n",
    "    \n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 40, {0: 1.0, 1: 4.0}),\n",
    "\n",
    "    \n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 3.0}),\n",
    "#     (f'Z_{repo_name}', 80, actual_vocab_size + 1, 24, True, 0.2, True, 0.0, [\"cp\"], 'accuracy', 128, 60, {0: 1.0, 1: 5.0}),\n",
    "]\n",
    "\n",
    "# Todo: Batch size, output dims, load_best_weights?\n",
    "# Todo: Add callback_patience\n",
    "# Todo: Transform into dict\n",
    "\n",
    "# Todo: Vary monitors\n",
    "\n",
    "all_scores = []\n",
    "len(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out = open(\"results.txt\", \"a\")\n",
    "# out.write(\"Name, max_length, vocab_size, batch_size, Balanced_Accuracy, Precision_Score, Recall_Score, F1_Score\")\n",
    "# out.write(\"Name, max_length, vocab_size, batch_size, trainable, dropout, sigmoid_out, Bal_Acc, Precision, Recall, F1_Score\")\n",
    "out.write(iteration_features + \", settings_hash, execution_time, Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1\")\n",
    "# out.write(str(iterations[0]))\n",
    "out.write(\"\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = zeros((vocab_size, output_dims))\n",
    "for i in range(vocab_size):\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "        embedding_vector = gensim_model.wv[str(i)]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Pad the context\n",
    "padded_inputs = pad_sequences(X_train_party, maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "padded_inputs_test = pad_sequences(X_test_party, maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_174repos_min50_max1000000_zhenhao\n",
      "Epoch 1/20\n",
      "54577/54577 [==============================] - 899s 16ms/step - loss: 0.3119 - f1_score: 0.2275 - val_loss: 0.1453 - val_f1_score: 0.2310\n",
      "Epoch 2/20\n",
      "54577/54577 [==============================] - 893s 16ms/step - loss: 0.3060 - f1_score: 0.2403 - val_loss: 0.1456 - val_f1_score: 0.2297\n",
      "Epoch 3/20\n",
      "54577/54577 [==============================] - 894s 16ms/step - loss: 0.3038 - f1_score: 0.2434 - val_loss: 0.1311 - val_f1_score: 0.2356\n",
      "Epoch 4/20\n",
      "54577/54577 [==============================] - 894s 16ms/step - loss: 0.3025 - f1_score: 0.2461 - val_loss: 0.1425 - val_f1_score: 0.2471\n",
      "Epoch 5/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3044 - f1_score: 0.2439 - val_loss: 0.1372 - val_f1_score: 0.2313\n",
      "Epoch 6/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3057 - f1_score: 0.2406 - val_loss: 0.1428 - val_f1_score: 0.2383\n",
      "Epoch 7/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3051 - f1_score: 0.2428 - val_loss: 0.1499 - val_f1_score: 0.2421\n",
      "Epoch 8/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3052 - f1_score: 0.2427 - val_loss: 0.1345 - val_f1_score: 0.2370\n",
      "Epoch 9/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3057 - f1_score: 0.2393 - val_loss: 0.1565 - val_f1_score: 0.2431\n",
      "Epoch 10/20\n",
      "54577/54577 [==============================] - 894s 16ms/step - loss: 0.3055 - f1_score: 0.2382 - val_loss: 0.1513 - val_f1_score: 0.2384\n",
      "Epoch 11/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3057 - f1_score: 0.2406 - val_loss: 0.1434 - val_f1_score: 0.2423\n",
      "Epoch 12/20\n",
      "54577/54577 [==============================] - 894s 16ms/step - loss: 0.3060 - f1_score: 0.2400 - val_loss: 0.1459 - val_f1_score: 0.2356\n",
      "Epoch 13/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3078 - f1_score: 0.2344 - val_loss: 0.1399 - val_f1_score: 0.2337\n",
      "Epoch 14/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3071 - f1_score: 0.2378 - val_loss: 0.1358 - val_f1_score: 0.2339\n",
      "Epoch 15/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3090 - f1_score: 0.2337 - val_loss: 0.1227 - val_f1_score: 0.2094\n",
      "Epoch 16/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3160 - f1_score: 0.2267 - val_loss: 0.1474 - val_f1_score: 0.2184\n",
      "Epoch 17/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3163 - f1_score: 0.2226 - val_loss: 0.1229 - val_f1_score: 0.1893\n",
      "Epoch 18/20\n",
      "54577/54577 [==============================] - 895s 16ms/step - loss: 0.3159 - f1_score: 0.2236 - val_loss: 0.1547 - val_f1_score: 0.2284\n",
      "Epoch 19/20\n",
      "54577/54577 [==============================] - 894s 16ms/step - loss: 0.3156 - f1_score: 0.2232 - val_loss: 0.1364 - val_f1_score: 0.2226\n",
      "Epoch 20/20\n",
      "54577/54577 [==============================] - 896s 16ms/step - loss: 0.3147 - f1_score: 0.2251 - val_loss: 0.1462 - val_f1_score: 0.2252\n"
     ]
    }
   ],
   "source": [
    "for iteration in iterations:\n",
    "    name, max_length, vocab_size, batch_size, trainable, dropout, sigmoid, val_split, callback, callback_monitor, num_nodes, num_epochs, class_weight, cmpltn_metrics = iteration\n",
    "    print(name)\n",
    "    \n",
    "    # Debug\n",
    "#     if num_epochs == 60:\n",
    "#         num_epochs = 40\n",
    "#     batch_size = 64\n",
    "    # /Debug\n",
    "    \n",
    "    settings_hash = int((hash(str(iteration)) ** 2) ** 0.5)\n",
    "    start = time.time()\n",
    "\n",
    "    # Build embedding matrix for different vocab_size (missing different output_dims)\n",
    "    if vocab_size != embedding_matrix.shape[0]:\n",
    "        print(\"Recomputing embedding matrix...\")\n",
    "        embedding_matrix = zeros((vocab_size, output_dims))\n",
    "        for i in range(vocab_size):\n",
    "            embedding_vector = None\n",
    "            try:\n",
    "                embedding_vector = gensim_model.wv[str(i)]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "     \n",
    "    # Pad the context for different max_length\n",
    "    if max_length != 80:\n",
    "        padded_inputs = pad_sequences(X_train_party, maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "        padded_inputs_test = pad_sequences(X_test_party, maxlen=max_length, value=0.0)  # 0.0 because it corresponds with <PAD>\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, output_dims, weights=[embedding_matrix], input_length=max_length, trainable=trainable))\n",
    "\n",
    "    if name.startswith(\"A\"):\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout))\n",
    "    elif name.startswith(\"B\"):\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout))\n",
    "        model.add(Dense(int(num_nodes / 4),activation='relu'))\n",
    "    elif name.startswith(\"C\"):\n",
    "        model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(128))\n",
    "    elif name.startswith(\"D\"):\n",
    "        model.add(CuDNNLSTM(num_nodes, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(num_nodes))\n",
    "        model.add(Dense(32,activation='relu'))\n",
    "    elif name.startswith(\"E\"):\n",
    "        model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "        model.add(Bidirectional(CuDNNLSTM(128)))\n",
    "        model.add(Dense(32,activation='relu'))\n",
    "    elif name.startswith(\"Y\"):\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout))\n",
    "        model.add(Dense(int(num_nodes / 4),activation='relu'))\n",
    "    elif name.startswith(\"Z\"):\n",
    "        model.add(tf.keras.layers.LSTM(num_nodes, dropout=dropout))\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "\n",
    "    # Best so far:   (Add dropout layer?)\n",
    "#     if name == \"bidirectional\":\n",
    "#         model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "#         model.add(Bidirectional(CuDNNLSTM(128)))\n",
    "#     else:\n",
    "#         model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "#         model.add(CuDNNLSTM(128))\n",
    "#     model.add(Dense(32,activation='relu'))\n",
    "\n",
    "    # Dropout with layer, terrible\n",
    "    # model.add(keras.layers.LSTM(128))\n",
    "    # model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "    if sigmoid:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    ######################\n",
    "    #### Complilation ####\n",
    "    ######################\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=#cmpltn_metrics\n",
    "                  [\n",
    "#         'accuracy',\n",
    "#         custom_f1,\n",
    "#         tf.keras.metrics.Recall(),\n",
    "#         tf.keras.metrics.BinaryAccuracy(),\n",
    "        tfa.metrics.F1Score(num_classes=1, threshold=0.5),\n",
    "#         tfa.metrics.F1Score(),\n",
    "                  ]\n",
    "                 )\n",
    "\n",
    "    \n",
    "#     if callback:\n",
    "#         # es = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=4)\n",
    "#         es = EarlyStopping(monitor=callback_monitor, mode='auto', verbose=1, patience=20, restore_best_weights=True)\n",
    "#         checkpoint = ModelCheckpoint(filepath=\"hybrid_model{epoch}\", monitor=\"accuracy\", mode=\"auto\",\n",
    "#                              save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "#         model.fit(padded_inputs, y_train if sigmoid else y_train_categorical,\n",
    "#                   epochs=num_epochs, batch_size=batch_size, validation_split=val_split, callbacks=[es])\n",
    "#     else:\n",
    "#         model.fit(padded_inputs, y_train if sigmoid else y_train_categorical,\n",
    "#                   epochs=num_epochs, batch_size=batch_size, validation_split=val_split)\n",
    "    # Remake:\n",
    "\n",
    "    callbacks = []\n",
    "    if \"es\" in callback:\n",
    "        # Default monitor \"val_loss\"?!\n",
    "        es = EarlyStopping(monitor=callback_monitor,\n",
    "#                            mode='auto',\n",
    "                           mode='max',\n",
    "                           verbose=1,\n",
    "                           patience=20,\n",
    "                           restore_best_weights=True)\n",
    "        callbacks.append(es)\n",
    "    if \"cp\" in callback:\n",
    "        # Default monitor \"val_loss\"\n",
    "        # No more epoch in filepath for loading the model weights after fit\n",
    "#         filepath = f\"zhenhao_models/{repo_name}/{settings_hash}/\" + \"epoch{epoch}\"\n",
    "        filepath = f\"zhenhao_models/{repo_name}/{settings_hash}/checkpoint\"\n",
    "        cp = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor=callback_monitor,\n",
    "#                              mode=\"auto\",\n",
    "                             mode=\"max\",\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             save_freq=\"epoch\")\n",
    "        callbacks.append(cp)\n",
    "    \n",
    "    if callbacks == []:\n",
    "        callbacks = None\n",
    "    \n",
    "    #############\n",
    "    #### FIT ####\n",
    "    #############\n",
    "    history = model.fit(padded_inputs,\n",
    "                        y_train if sigmoid else y_train_categorical,\n",
    "                        epochs=num_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(padded_inputs_test, y_test),\n",
    "                        validation_split=val_split,\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weight)\n",
    "\n",
    "    \n",
    "#     if name == f'Z_{repo_name}_cploaded':\n",
    "#         print(\"Loading best weights...\")\n",
    "#         model.load_weights(filepath)\n",
    "    \n",
    "    # Predict\n",
    "    # pred_train= model.predict(padded_inputs)\n",
    "    pred_test= model.predict(padded_inputs_test, batch_size=batch_size)\n",
    "    if sigmoid:\n",
    "        y_pred = np.round(pred_test)\n",
    "    else:\n",
    "        y_pred = []\n",
    "        for zero, one in pred_test:\n",
    "            if zero > 0.5:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "    \n",
    "    # Now load the best weights and test again\n",
    "    model.load_weights(filepath)\n",
    "    best_pred_test= model.predict(padded_inputs_test, batch_size=batch_size)\n",
    "    if sigmoid:\n",
    "        best_y_pred = np.round(best_pred_test)\n",
    "    else:\n",
    "        best_y_pred = []\n",
    "        for zero, one in best_pred_test:\n",
    "            if zero > 0.5:\n",
    "                best_y_pred.append(0)\n",
    "            else:\n",
    "                best_y_pred.append(1)\n",
    "\n",
    "    end = time.time()\n",
    "    execution_time = int(end - start)\n",
    "    \n",
    "    # Scores\n",
    "    scores = [\n",
    "        name,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        trainable,\n",
    "        dropout,\n",
    "        sigmoid,\n",
    "        val_split,\n",
    "        callback,\n",
    "        callback_monitor,\n",
    "        num_nodes,\n",
    "        num_epochs,\n",
    "        class_weight,\n",
    "        list(map(lambda x: x.__name__ if callable(x) else x, cmpltn_metrics)),\n",
    "        settings_hash,\n",
    "        execution_time,\n",
    "        f\"{balanced_accuracy_score(y_test, y_pred):.2f}\"[2:],\n",
    "        f\"{precision_score(y_test, y_pred):.2f}\"[2:],\n",
    "        f\"{recall_score(y_test, y_pred):.2f}\"[2:],\n",
    "        f\"{f1_score(y_test, y_pred):.3f}\"[2:],\n",
    "        f\"{balanced_accuracy_score(y_test, best_y_pred):.2f}\"[2:],\n",
    "        f\"{precision_score(y_test, best_y_pred):.2f}\"[2:],\n",
    "        f\"{recall_score(y_test, best_y_pred):.2f}\"[2:],\n",
    "        f\"{f1_score(y_test, best_y_pred):.3f}\"[2:],\n",
    "    ]\n",
    "    out = open(\"results.txt\", \"a\")\n",
    "    out.write(str(scores).replace(\"'\", \"\")[1:-1])\n",
    "    out.write(\"\\n\")\n",
    "    out.close()\n",
    "#     all_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_data': None,\n",
       " 'model': <keras.engine.sequential.Sequential at 0x7fa30527bbe0>,\n",
       " '_chief_worker_only': None,\n",
       " '_supports_tf_logs': False,\n",
       " 'history': {'loss': [0.3096500039100647,\n",
       "   0.3010702431201935,\n",
       "   0.2996620237827301,\n",
       "   0.2972530722618103,\n",
       "   0.2955453395843506,\n",
       "   0.29379507899284363,\n",
       "   0.2926989793777466,\n",
       "   0.29235410690307617,\n",
       "   0.2913242280483246,\n",
       "   0.290630966424942,\n",
       "   0.28983640670776367,\n",
       "   0.2896806299686432,\n",
       "   0.2892162501811981,\n",
       "   0.28887367248535156,\n",
       "   0.2887428402900696,\n",
       "   0.2896438539028168,\n",
       "   0.2883419990539551,\n",
       "   0.2894187271595001,\n",
       "   0.2880391776561737,\n",
       "   0.28797587752342224],\n",
       "  'f1_score': [array([0.23132716], dtype=float32),\n",
       "   array([0.2498759], dtype=float32),\n",
       "   array([0.25386375], dtype=float32),\n",
       "   array([0.25819632], dtype=float32),\n",
       "   array([0.26188293], dtype=float32),\n",
       "   array([0.26594236], dtype=float32),\n",
       "   array([0.26852974], dtype=float32),\n",
       "   array([0.26866344], dtype=float32),\n",
       "   array([0.27133986], dtype=float32),\n",
       "   array([0.2717408], dtype=float32),\n",
       "   array([0.27632678], dtype=float32),\n",
       "   array([0.27520904], dtype=float32),\n",
       "   array([0.27797687], dtype=float32),\n",
       "   array([0.27783674], dtype=float32),\n",
       "   array([0.27972952], dtype=float32),\n",
       "   array([0.2789231], dtype=float32),\n",
       "   array([0.2801838], dtype=float32),\n",
       "   array([0.27803206], dtype=float32),\n",
       "   array([0.2784471], dtype=float32),\n",
       "   array([0.2805675], dtype=float32)],\n",
       "  'val_loss': [0.13142728805541992,\n",
       "   0.1314118355512619,\n",
       "   0.1472981721162796,\n",
       "   0.1424086093902588,\n",
       "   0.1367315798997879,\n",
       "   0.14274181425571442,\n",
       "   0.13373617827892303,\n",
       "   0.14209212362766266,\n",
       "   0.1391085386276245,\n",
       "   0.13086490333080292,\n",
       "   0.13587254285812378,\n",
       "   0.14933902025222778,\n",
       "   0.15013888478279114,\n",
       "   0.1345212757587433,\n",
       "   0.13264070451259613,\n",
       "   0.13128334283828735,\n",
       "   0.1361483931541443,\n",
       "   0.14301052689552307,\n",
       "   0.13397540152072906,\n",
       "   0.14870066940784454],\n",
       "  'val_f1_score': [array([0.23337619], dtype=float32),\n",
       "   array([0.2500108], dtype=float32),\n",
       "   array([0.25244603], dtype=float32),\n",
       "   array([0.2564598], dtype=float32),\n",
       "   array([0.25817657], dtype=float32),\n",
       "   array([0.2636161], dtype=float32),\n",
       "   array([0.2581578], dtype=float32),\n",
       "   array([0.26502958], dtype=float32),\n",
       "   array([0.26400873], dtype=float32),\n",
       "   array([0.265776], dtype=float32),\n",
       "   array([0.2663149], dtype=float32),\n",
       "   array([0.26564512], dtype=float32),\n",
       "   array([0.26873457], dtype=float32),\n",
       "   array([0.26930663], dtype=float32),\n",
       "   array([0.26920626], dtype=float32),\n",
       "   array([0.27000916], dtype=float32),\n",
       "   array([0.27115297], dtype=float32),\n",
       "   array([0.27406514], dtype=float32),\n",
       "   array([0.27317074], dtype=float32),\n",
       "   array([0.27204946], dtype=float32)]},\n",
       " 'params': {'verbose': 1, 'epochs': 20, 'steps': 27289},\n",
       " 'epoch': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}