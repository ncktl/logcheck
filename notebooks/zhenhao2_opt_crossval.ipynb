{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import time\n",
    "from notebook_helper import MyCorpus, build_model, build_callbacks, build_embedding_matrix\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Keras specific\n",
    "\n",
    "#### CHANGED from import keras:\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "#####\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, CuDNNLSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# Gemsim\n",
    "import gensim.models\n",
    "from gensim import utils\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "After:\n",
      " [PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\\n\" ,tf.config.get_visible_devices('GPU'))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "except IndexError as e:\n",
    "    pass\n",
    "print(\"After:\\n\" ,tf.config.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the data\n",
    "min_val = 50\n",
    "repo_name = f\"174repos_min{min_val}_max1000000_zhenhao\"\n",
    "# repo_name = f\"300repos_min{min_val}_max1000000_zhenhao\"\n",
    "# repo_name = f\"combination_zhenhao\"\n",
    "df = pd.read_csv('../features/'+ repo_name +'.csv')\n",
    "\n",
    "# Remove errors\n",
    "df = df[df.type != 'b']\n",
    "\n",
    "no_log_cnt, log_cnt = df['contains_logging'].value_counts()\n",
    "par_vec_cnt = no_log_cnt + log_cnt\n",
    "log_ratio = log_cnt / par_vec_cnt\n",
    "print(f\"Number of parameter vecs:\\t\\t{par_vec_cnt}\")\n",
    "print(f\"without logging (negatives):\\t{no_log_cnt}\")\n",
    "print(f\"with logging (positives):\\t\\t{log_cnt}\")\n",
    "print(f\"Log ratio:\\t\\t\\t\\t\\t\\t{log_ratio * 100:.2f}%\")\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X = df.context\n",
    "# Split the context string into list of characters,\n",
    "# then replace the characters with their index in the alphabet (a-zA-Z) as strings\n",
    "# E.g. 'cad' -> ['2','0','3']\n",
    "X = [list(map(lambda y: str(ascii_letters.index(y)), list(x))) for x in X]\n",
    "# Build the Word2Vec Model\n",
    "sentences = MyCorpus(X)\n",
    "gensim_model = gensim.models.Word2Vec(sentences=sentences, min_count=1)\n",
    "actual_vocab_size = len(gensim_model.wv.key_to_index)\n",
    "\n",
    "y = df.contains_logging\n",
    "\n",
    "# Default values\n",
    "default_output_dims = 100\n",
    "default_max_length = 80\n",
    "default_vocab_size = actual_vocab_size + 1\n",
    "default_batch_size = 24\n",
    "default_trainable = False\n",
    "default_dropout = 0.2\n",
    "default_val_split = 0.0\n",
    "default_callback = [\"cp\"]\n",
    "default_callback_monitor = 'val_f1_score'\n",
    "default_num_nodes = 128\n",
    "default_num_epochs = 100\n",
    "default_class_weight = {0: 1.0, 1: 5.0}\n",
    "default_cmpltn_metrics = [tfa.metrics.F1Score(num_classes=1, threshold=0.5)]\n",
    "\n",
    "# Cross-validation settings\n",
    "n_splits = 3\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = build_embedding_matrix(default_vocab_size, default_output_dims, gensim_model)\n",
    "\n",
    "# Pad the context\n",
    "X_unpadded = np.array(X, dtype=object)\n",
    "X = pad_sequences(X_unpadded, maxlen=default_max_length, value=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iteration_features =  \"Name, max_length, vocab_size, batch_size, trainable, dropout, val_split, callback, callback_monitor, num_nodes, num_epochs, class_weight, cmpltn_metrics\"\n",
    "iterations = [\n",
    "# B_ is best but Z_ is faster and what Zhenhao used\n",
    "# Test model.fit(validation_data=(padded_inputs_test, y_test)) and callback_monitor='val_f1_score'\n",
    "        (f'Z_{repo_name}_cv{n_splits}', 80, actual_vocab_size + 1, 32, True, 0.2, 0.0, [\"es\", \"cp\"], 'val_f1_score', 128, 20, {0: 1.0, 1: 5.0}, default_cmpltn_metrics),\n",
    "# TODO: Do Crossvalidation stratified shuffled fold testing with high batch size to compensate\n",
    "\n",
    "]\n",
    "\n",
    "# Todo: Batch size, output dims, load_best_weights?\n",
    "# Todo: Add callback_patience\n",
    "# Todo: Transform into dict\n",
    "\n",
    "all_scores = []\n",
    "len(iterations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = open(\"results.txt\", \"a\")\n",
    "out.write(iteration_features + \", settings_hash, execution_time, Final_Bal_Acc, Final_Prec, Final_Recall, Final_F1, Best_Bal_Acc, Best_Prec, Best_Recall, Best_F1\")\n",
    "# out.write(str(iterations[0]))\n",
    "out.write(\"\\n\")\n",
    "out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for iteration in iterations:\n",
    "    name, max_length, vocab_size, batch_size, trainable, dropout, val_split, callback, callback_monitor, num_nodes, num_epochs, class_weight, cmpltn_metrics = iteration\n",
    "    print(name)\n",
    "    settings_hash = int((hash(str(iteration)) ** 2) ** 0.5)\n",
    "    start = time.time()\n",
    "\n",
    "    # Rebuild embedding matrix in case of changed vocab_size (missing changed output_dims)\n",
    "    if vocab_size != embedding_matrix.shape[0]:\n",
    "        embedding_matrix = build_embedding_matrix(vocab_size, default_output_dims, gensim_model)\n",
    "\n",
    "     # Pad the context for different max_length\n",
    "    if max_length != X.shape[1]:\n",
    "        X = pad_sequences(X_unpadded, maxlen=max_length, value=0.0)\n",
    "\n",
    "    final_bal_acc_all, final_precision_all, final_recall_all, final_f1_all = [], [], [], []\n",
    "    best_bal_acc_all, best_precision_all, best_recall_all, best_f1_all = [], [], [], []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "    for k_fold, (train_indices, test_indices) in enumerate(skf.split(X=X, y=y)):\n",
    "        print(f\"Starting fold {k_fold + 1} of {n_splits}.\")\n",
    "        X_train, y_train = X[train_indices], y.iloc[train_indices]\n",
    "        X_test, y_test = X[test_indices], y.iloc[test_indices]\n",
    "\n",
    "        # Build the model\n",
    "        model = build_model(name, vocab_size, default_output_dims, embedding_matrix, max_length, trainable, num_nodes, dropout)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=cmpltn_metrics)\n",
    "\n",
    "        # Build the callbacks\n",
    "        callbacks, model_cp_filepath = build_callbacks(callback, callback_monitor, repo_name, settings_hash, k_fold)\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            validation_split=val_split,\n",
    "                            callbacks=callbacks,\n",
    "                            class_weight=class_weight)\n",
    "\n",
    "        # Predict on test data\n",
    "        pred_test= model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.round(pred_test)\n",
    "        final_bal_acc_all.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        final_precision_all.append(precision_score(y_test, y_pred))\n",
    "        final_recall_all.append(recall_score(y_test, y_pred))\n",
    "        final_f1_all.append(f1_score(y_test, y_pred))\n",
    "\n",
    "        if \"cp\" in callback:\n",
    "            # Now load the best weights and predict on test data again\n",
    "            model.load_weights(model_cp_filepath)\n",
    "            best_pred_test= model.predict(X_test, batch_size=batch_size)\n",
    "            best_y_pred = np.round(best_pred_test)\n",
    "            best_bal_acc_all.append(balanced_accuracy_score(y_test, best_y_pred))\n",
    "            best_precision_all.append(precision_score(y_test, best_y_pred))\n",
    "            best_recall_all.append(recall_score(y_test, best_y_pred))\n",
    "            best_f1_all.append(f1_score(y_test, best_y_pred))\n",
    "\n",
    "    end = time.time()\n",
    "    execution_time = int(end - start)\n",
    "\n",
    "    # Scores\n",
    "    scores = [\n",
    "        name,\n",
    "        max_length,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        trainable,\n",
    "        dropout,\n",
    "        val_split,\n",
    "        callback,\n",
    "        callback_monitor,\n",
    "        num_nodes,\n",
    "        num_epochs,\n",
    "        class_weight,\n",
    "        list(map(lambda x: x.name if callable(x) else x, cmpltn_metrics)),\n",
    "        settings_hash,\n",
    "        execution_time,\n",
    "        f\"{np.mean(final_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(final_f1_all, axis=0):.3f}\"[2:],\n",
    "        f\"{np.mean(best_bal_acc_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_precision_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_recall_all, axis=0):.2f}\"[2:],\n",
    "        f\"{np.mean(best_f1_all, axis=0):.3f}\"[2:],\n",
    "    ]\n",
    "\n",
    "    out = open(\"results.txt\", \"a\")\n",
    "    out.write(str(scores).replace(\"'\", \"\")[1:-1])\n",
    "    out.write(\"\\n\")\n",
    "    out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vars(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}